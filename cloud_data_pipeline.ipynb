{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â„ï¸ Cloud Data Ingestion into Snowflake\n",
    "\n",
    "**PyData Boston 2025 - From Notebook to Pipeline: Hands-On Data Engineering with Python**\n",
    "\n",
    "In this notebook, you'll build a complete data pipeline that:\n",
    "- ðŸ“¥ Ingests ~1 billion rows from CSV files in AWS S3\n",
    "- ðŸ”„ Transforms data using Snowflake Dynamic Tables\n",
    "- ðŸ“Š Creates aggregated business metrics with automatic incremental refresh\n",
    "- ðŸ¤– Optionally enables AI-powered insights via Snowflake Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Setup Zone\n",
    "\n",
    "Choose your preferred environment setup below:\n",
    "\n",
    "| Option | Best For | Description |\n",
    "|--------|----------|-------------|\n",
    "| **Option A** | Intermediate/Advanced | Local development with virtual environments |\n",
    "| **Option B** | Beginners | Run directly in Snowflake's hosted notebooks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ–¥ï¸ Option A: Local Development Setup\n",
    "\n",
    "**Recommended for:** Users comfortable with terminal, Python environments, and package management.\n",
    "\n",
    "#### Step 1: Clone the Repository\n",
    "```bash\n",
    "git clone https://github.com/Snowflake-Labs/pydata_boston_2025_notebook_to_pipeline.git\n",
    "cd pydata_boston_2025_notebook_to_pipeline\n",
    "```\n",
    "\n",
    "#### Step 2: Create Virtual Environment\n",
    "```bash\n",
    "python3 -m venv pydataboston_2025\n",
    "\n",
    "# Activate (macOS/Linux)\n",
    "source pydataboston_2025/bin/activate\n",
    "\n",
    "# Activate (Windows)\n",
    "pydataboston_2025\\Scripts\\activate\n",
    "```\n",
    "\n",
    "#### Step 3: Install Dependencies\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### Step 4: Verify Installation\n",
    "```python\n",
    "python3 -c \"from snowflake.snowpark import Session; from snowflake.core import Root; print('âœ… Installation successful!')\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â˜ï¸ Option B: Snowflake Notebooks Setup\n",
    "\n",
    "**Recommended for:** Beginners or those who prefer a managed environment.\n",
    "\n",
    "1. **Log in** to your [Snowflake account](https://app.snowflake.com/)\n",
    "2. **Navigate** to **Projects** â†’ **Notebooks**\n",
    "3. **Import notebook:**\n",
    "   - Click **+ Notebook** dropdown â†’ **Import .ipynb file**\n",
    "   - Name: `cloud_data_pipeline`\n",
    "   - Database: `SNOWFLAKE_LEARNING_DB`\n",
    "   - Schema: `PUBLIC`\n",
    "   - Runtime: `Run on warehouse`\n",
    "4. **Install packages** via the **Packages** dropdown:\n",
    "   - Search and add: `snowflake`\n",
    "   - Search and add: `snowflake-snowpark-python`\n",
    "5. **Skip to the imports cell** (Cell 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… End of Setup Zone\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Import Libraries\n",
    "\n",
    "We'll use two main Snowflake packages:\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|---------|\n",
    "| `snowflake.core` | Python API for managing Snowflake objects (databases, schemas, tables) |\n",
    "| `snowflake.snowpark` | DataFrame API for data transformations using Snowflake's compute engine |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session # For Snowflake compute engine connection\n",
    "from snowflake.core import Root, CreateMode # For core operations\n",
    "from snowflake.core.database import Database # For database operations\n",
    "from snowflake.core.schema import Schema # For schema operations\n",
    "from snowflake.core.warehouse import Warehouse # For compute engine operations\n",
    "from snowflake.core.table import Table, TableColumn, TableCollection # For table operations\n",
    "from snowflake.core.role import Role, Securable # For role operations\n",
    "from snowflake.core.stage import Stage, StageDirectoryTable # For stage operations\n",
    "from snowflake.core.dynamic_table import DynamicTable, DownstreamLag, UserDefinedLag, DynamicTableCollection # For dynamic table operations\n",
    "from snowflake.snowpark.types import * # For defining DataFrame schema\n",
    "from snowflake.snowpark import functions as F # For DataFrame functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Œ Connect to Snowflake\n",
    "\n",
    "We'll connect using Snowflake's compute engine for distributed processingâ€”no local memory limitations!\n",
    "\n",
    "**Connection Methods Available:**\n",
    "1. âœ… Connection parameters dictionary *(used in this tutorial)*\n",
    "2. Snowflake CLI configuration file\n",
    "3. Environment variables or external authentication\n",
    "\n",
    "> âš ï¸ **Security Note:** Never commit credentials to version control in production. Use environment variables or secret managers instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snowflake Login](./img/snowflake_login.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OPTION A ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„ï¸ Connected to Snowflake account: \"OWVFCQY-OUB97142\"\n",
      "Current role: \"ACCOUNTADMIN\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Build connection parameters from environment variables\n",
    "connection_parameters = {\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "}\n",
    "\n",
    "# Create a session\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(f\"â„ï¸ Connected to Snowflake account: {session.get_current_account()}\")\n",
    "print(f\"Current role: {session.get_current_role()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OPTION B ONLY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.context import get_active_session\n",
    "# session = get_active_session()\n",
    "\n",
    "# print(f\"â„ï¸ Connected to Snowflake account: {session.get_current_account()}\")\n",
    "# print(f\"Current role: {session.get_current_role()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Root API Object\n",
    "\n",
    "The `Root` object is the entry point for Snowflake's Python APIâ€”it provides access to all database objects and operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root API object created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create root object from Root():\n",
    "root = Root(session)\n",
    "print(\"Root API object created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Create Lab Role (RBAC)\n",
    "\n",
    "We'll create a dedicated role following the **principle of least privilege**â€”users should have only the minimum permissions required for their tasks.\n",
    "\n",
    "> **RBAC (Role-Based Access Control)** is a security best practice in data platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role 'pydata_lab_role' created successfully\n",
      "Privileges granted to pydata_lab_role\n"
     ]
    }
   ],
   "source": [
    "# Create lab role using Python API\n",
    "pydata_lab_role = Role(name=\"pydata_lab_role\")\n",
    "root.roles.create(pydata_lab_role, mode=CreateMode.if_not_exists)\n",
    "print(\"Role 'pydata_lab_role' created successfully\")\n",
    "\n",
    "# Grant role to SYSADMIN (SQL required for role grants)\n",
    "session.sql(\"GRANT ROLE pydata_lab_role TO ROLE SYSADMIN\").collect()\n",
    "\n",
    "# Grant necessary privileges to role (SQL required for account-level grants)\n",
    "session.sql(\"GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE pydata_lab_role\").collect()\n",
    "session.sql(\"GRANT CREATE DATABASE ON ACCOUNT TO ROLE pydata_lab_role\").collect()\n",
    "\n",
    "print(\"Privileges granted to pydata_lab_role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to Lab Role\n",
    "\n",
    "Assume the `pydata_lab_role` for all subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to role: \"PYDATA_LAB_ROLE\"\n"
     ]
    }
   ],
   "source": [
    "session.use_role(\"pydata_lab_role\")\n",
    "print(f\"Switched to role: {session.get_current_role()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Define Data Objects ðŸ“\n",
    "\n",
    "In this section, we'll create the foundational infrastructure:\n",
    "- Database and schemas\n",
    "- Compute warehouse\n",
    "- External stage (S3 connection)\n",
    "- Raw data tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—„ï¸ Create Database\n",
    "\n",
    "Create `tasty_bytes_db` to contain all Tasty Bytes sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'tasty_bytes_db' created successfully\n",
      "Session is now using database: \"TASTY_BYTES_DB\"\n"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "database_name = \"tasty_bytes_db\"\n",
    "new_database = Database(name=database_name)\n",
    "root.databases.create(new_database, mode=CreateMode.or_replace)\n",
    "\n",
    "print(f\"Database '{database_name}' created successfully\")\n",
    "\n",
    "# Set database context\n",
    "session.use_database(database_name)\n",
    "print(f\"Session is now using database: {session.get_current_database()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ Create Schemas\n",
    "\n",
    "Schemas logically group related data. We'll create two:\n",
    "\n",
    "| Schema | Purpose |\n",
    "|--------|---------|\n",
    "| `raw` | Source data from CSV files |\n",
    "| `analytics` | Transformed, business-ready data |\n",
    "\n",
    "> ðŸ’¡ **Best Practice:** Separating raw and transformed data is a common pattern in data engineering.\n",
    "\n",
    "![Schemas Creation](./img/schemas_creation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema 'raw' created successfully\n",
      "Schema 'analytics' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Get database reference\n",
    "db = root.databases[database_name]\n",
    "\n",
    "# Create raw schema\n",
    "raw_schema = Schema(name=\"raw\")\n",
    "db.schemas.create(raw_schema, mode=CreateMode.or_replace)\n",
    "print(\"Schema 'raw' created successfully\")\n",
    "\n",
    "# Create analytics schema using Python API:\n",
    "analytics_schema = Schema(name=\"analytics\")\n",
    "db.schemas.create(analytics_schema, mode=CreateMode.or_replace)\n",
    "print(\"Schema 'analytics' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Create Compute Warehouse\n",
    "\n",
    "A virtual warehouse provides compute resources for query execution. We'll create an **X-Large** warehouse with auto-suspend for cost optimization.\n",
    "\n",
    "![Virtual Warehouse](./img/virtual_wh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse 'pydata_lab_wh' created successfully\n",
      "Granted USAGE on warehouse to pydata_lab_role\n",
      "Using virtual warehouse: \"PYDATA_LAB_WH\"\n"
     ]
    }
   ],
   "source": [
    "# Create compute resource\n",
    "virtual_warehouse_name = \"pydata_lab_wh\"\n",
    "\n",
    "new_warehouse = Warehouse(\n",
    "    name=virtual_warehouse_name,\n",
    "    warehouse_size=\"XLARGE\"\n",
    ")\n",
    "\n",
    "root.warehouses.create(new_warehouse, mode=CreateMode.or_replace)\n",
    "print(f\"Warehouse '{virtual_warehouse_name}' created successfully\")\n",
    "\n",
    "# Grant usage on warehouse to pydata_lab_role\n",
    "root.roles[\"pydata_lab_role\"].grant_privileges(\n",
    "    privileges=[\"USAGE\"],\n",
    "    securable_type=\"WAREHOUSE\",\n",
    "    securable=Securable(name=virtual_warehouse_name)\n",
    ")\n",
    "print(f\"Granted USAGE on warehouse to pydata_lab_role\")\n",
    "\n",
    "# Use the warehouse\n",
    "session.use_warehouse(virtual_warehouse_name)\n",
    "print(f\"Using virtual warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¤ Create External Stage\n",
    "\n",
    "Stages define locations where data files are stored. We'll create an external stage pointing to a public AWS S3 bucket containing our raw CSV data.\n",
    "\n",
    "> ðŸ’¡ **Common Pattern:** Land raw data files in cloud object storage (S3, Azure Blob, GCS), then ingest into your data platform.\n",
    "\n",
    "![Stages](./img/stages.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External stage 'tasty_bytes_stage' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create external stage\n",
    "tasty_bytes_stage = Stage(\n",
    "    name=\"tasty_bytes_stage\",\n",
    "    url=\"s3://sfquickstarts/tasty-bytes-builder-education/\",\n",
    "    directory_table=StageDirectoryTable(enable=True)\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"raw\"].stages.create(\n",
    "    tasty_bytes_stage,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "print(\"External stage 'tasty_bytes_stage' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Create Raw Tables\n",
    "\n",
    "Define tables to hold our raw data (~1 billion rows total):\n",
    "\n",
    "| Table | Description | Row Count |\n",
    "|-------|-------------|-----------|\n",
    "| `order_header` | Order-level information | ~248M |\n",
    "| `order_detail` | Line item information | ~618M |\n",
    "| `menu` | Product catalog | ~287 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'order_header' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Get schema reference\n",
    "raw_schema_ref = db.schemas[\"raw\"]\n",
    "\n",
    "# Define order_header table structure\n",
    "order_header_columns = [\n",
    "    TableColumn(name=\"order_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"truck_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"location_id\", datatype=\"FLOAT\"),\n",
    "    TableColumn(name=\"customer_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"discount_id\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"shift_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"shift_start_time\", datatype=\"TIME(9)\"),\n",
    "    TableColumn(name=\"shift_end_time\", datatype=\"TIME(9)\"),\n",
    "    TableColumn(name=\"order_channel\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_ts\", datatype=\"TIMESTAMP_NTZ(9)\"),\n",
    "    TableColumn(name=\"served_ts\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_currency\", datatype=\"VARCHAR(3)\"),\n",
    "    TableColumn(name=\"order_amount\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"order_tax_amount\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_discount_amount\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_total\", datatype=\"NUMBER(38,4)\")\n",
    "]\n",
    "\n",
    "# Create order_header table using Python API\n",
    "order_header_table = Table(\n",
    "    name=\"order_header\",\n",
    "    columns=order_header_columns\n",
    ")\n",
    "\n",
    "raw_schema_ref.tables.create(order_header_table, mode=CreateMode.or_replace)\n",
    "print(\"Table 'order_header' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'order_detail' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Define order_detail table structure\n",
    "order_detail_columns = [\n",
    "    TableColumn(name=\"order_detail_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"order_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"menu_item_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"discount_id\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"line_number\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"quantity\", datatype=\"NUMBER(5,0)\"),\n",
    "    TableColumn(name=\"unit_price\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"price\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"order_item_discount_amount\", datatype=\"VARCHAR(16777216)\")\n",
    "]\n",
    "\n",
    "# Create order_detail table\n",
    "order_detail_table = Table(\n",
    "    name=\"order_detail\",\n",
    "    columns=order_detail_columns\n",
    ")\n",
    "\n",
    "raw_schema_ref.tables.create(order_detail_table, mode=CreateMode.or_replace)\n",
    "print(\"Table 'order_detail' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'menu' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Define menu table structure\n",
    "menu_columns = [\n",
    "    TableColumn(name=\"menu_id\", datatype=\"NUMBER(19,0)\"),\n",
    "    TableColumn(name=\"menu_type_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"menu_type\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"truck_brand_name\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"menu_item_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"menu_item_name\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"item_category\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"item_subcategory\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"cost_of_goods_usd\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"sale_price_usd\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"menu_item_health_metrics_obj\", datatype=\"VARIANT\")\n",
    "]\n",
    "\n",
    "# Create menu table\n",
    "menu_table = Table(\n",
    "    name=\"menu\",\n",
    "    columns=menu_columns\n",
    ")\n",
    "\n",
    "raw_schema_ref.tables.create(menu_table, mode=CreateMode.or_replace)\n",
    "print(\"Table 'menu' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Ingest Data ðŸ“¥\n",
    "\n",
    "Load data from CSV files in our external stage into the raw tables using Snowpark's `copy_into_table` method.\n",
    "\n",
    "> â±ï¸ **Note:** Loading ~1 billion records may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading order_header data...\n",
      "Loaded 179 batch(es) into order_header\n"
     ]
    }
   ],
   "source": [
    "# Load order_header data using Snowpark copy_into_table\n",
    "print(\"Loading order_header data...\")\n",
    "\n",
    "order_header_df = session.read.csv(\"@tasty_bytes_db.raw.tasty_bytes_stage/raw_pos/order_header/\")\n",
    "\n",
    "# Map $1, $2, $3... to the actual columns by position\n",
    "result = order_header_df.copy_into_table(\n",
    "    \"tasty_bytes_db.raw.order_header\",  \n",
    "    target_columns=[\"order_id\", \"truck_id\", \"location_id\", \"customer_id\", \"discount_id\", \n",
    "                    \"shift_id\", \"shift_start_time\", \"shift_end_time\", \"order_channel\", \n",
    "                    \"order_ts\", \"served_ts\", \"order_currency\", \"order_amount\", \n",
    "                    \"order_tax_amount\", \"order_discount_amount\", \"order_total\"],\n",
    "    transformations=[\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \n",
    "                     \"$9\", \"$10\", \"$11\", \"$12\", \"$13\", \"$14\", \"$15\", \"$16\"]\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(result)} batch(es) into order_header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading order_detail data...\n",
      "Successfully loaded 270 batch(es) into order_detail\n"
     ]
    }
   ],
   "source": [
    "# Load order_detail data using Snowpark copy_into_table\n",
    "print(\"Loading order_detail data...\")\n",
    "\n",
    "order_detail_df = session.read.csv(\"@tasty_bytes_db.raw.tasty_bytes_stage/raw_pos/order_detail/\")\n",
    "\n",
    "result = order_detail_df.copy_into_table(\n",
    "    \"tasty_bytes_db.raw.order_detail\",\n",
    "    target_columns=[\"order_detail_id\", \"order_id\", \"menu_item_id\", \"discount_id\", \n",
    "                    \"line_number\", \"quantity\", \"unit_price\", \"price\", \"order_item_discount_amount\"],\n",
    "    transformations=[\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \"$9\"]\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(result)} batch(es) into order_detail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading menu data...\n",
      "Successfully loaded 1 batch(es) into menu\n"
     ]
    }
   ],
   "source": [
    "# Load menu data using Snowpark copy_into_table\n",
    "print(\"Loading menu data...\")\n",
    "\n",
    "menu_df = session.read.csv(\"@tasty_bytes_db.raw.tasty_bytes_stage/raw_pos/menu/\")\n",
    "\n",
    "result = menu_df.copy_into_table(\n",
    "    \"tasty_bytes_db.raw.menu\",\n",
    "    target_columns=[\"menu_id\", \"menu_type_id\", \"menu_type\", \"truck_brand_name\", \"menu_item_id\",\n",
    "                    \"menu_item_name\", \"item_category\", \"item_subcategory\", \"cost_of_goods_usd\",\n",
    "                    \"sale_price_usd\", \"menu_item_health_metrics_obj\"],\n",
    "    transformations=[\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \"$9\", \"$10\", \"$11\"]\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(result)} batch(es) into menu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Data Loading\n",
    "\n",
    "Confirm data was loaded correctly by checking row counts using Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_header table: 248,201,269 rows\n",
      "order_detail table: 673,655,465 rows\n",
      "menu table: 100 rows\n"
     ]
    }
   ],
   "source": [
    "# Check row counts using Snowpark\n",
    "order_header_count = session.table(\"tasty_bytes_db.raw.order_header\").count()\n",
    "order_detail_count = session.table(\"tasty_bytes_db.raw.order_detail\").count()\n",
    "menu_count = session.table(\"tasty_bytes_db.raw.menu\").count()\n",
    "\n",
    "print(f\"order_header table: {order_header_count:,} rows\")\n",
    "print(f\"order_detail table: {order_detail_count:,} rows\")\n",
    "print(f\"menu table: {menu_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from order_header:\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"TRUCK_ID\"  |\"LOCATION_ID\"  |\"CUSTOMER_ID\"  |\"DISCOUNT_ID\"  |\"SHIFT_ID\"  |\"SHIFT_START_TIME\"  |\"SHIFT_END_TIME\"  |\"ORDER_CHANNEL\"  |\"ORDER_TS\"           |\"SERVED_TS\"  |\"ORDER_CURRENCY\"  |\"ORDER_AMOUNT\"  |\"ORDER_TAX_AMOUNT\"  |\"ORDER_DISCOUNT_AMOUNT\"  |\"ORDER_TOTAL\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|354065609   |314         |7083.0         |NULL           |NULL           |200350252   |09:00:00            |15:00:00          |NULL             |2022-06-29 14:42:49  |NULL         |USD               |107.0000        |NULL                |NULL                     |107.0000       |\n",
      "|354065610   |314         |7083.0         |NULL           |NULL           |200350252   |09:00:00            |15:00:00          |NULL             |2022-06-29 14:43:52  |NULL         |USD               |37.0000         |NULL                |NULL                     |37.0000        |\n",
      "|354065611   |314         |7083.0         |NULL           |NULL           |200350252   |09:00:00            |15:00:00          |NULL             |2022-06-29 14:44:04  |NULL         |USD               |18.0000         |NULL                |NULL                     |18.0000        |\n",
      "|354065612   |314         |7083.0         |NULL           |NULL           |200350252   |09:00:00            |15:00:00          |NULL             |2022-06-29 14:44:59  |NULL         |USD               |89.0000         |NULL                |NULL                     |89.0000        |\n",
      "|354065613   |314         |7083.0         |NULL           |NULL           |200350252   |09:00:00            |15:00:00          |NULL             |2022-06-29 14:45:23  |NULL         |USD               |151.0000        |NULL                |NULL                     |151.0000       |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View sample data from order_header using Snowpark (you can do this with the other table names too)\n",
    "print(\"Sample data from order_header:\")\n",
    "session.table(\"tasty_bytes_db.raw.order_header\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Transform Raw Data ðŸ”„\n",
    "\n",
    "Build a 3-tier transformation pipeline using **Snowflake Dynamic Tables**â€”Snowflake's declarative approach to data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ What are Dynamic Tables?\n",
    "\n",
    "Dynamic Tables let you define the **desired end state** of your dataâ€”Snowflake handles the rest!\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| **Automatic Scheduling** | No manual orchestration needed |\n",
    "| **Incremental Refresh** | Processes only changed rows |\n",
    "| **Dependency Tracking** | Native DAG visualization in UI |\n",
    "\n",
    "> **Key Concept:** Define your transformations declaratively, and Snowflake manages the pipeline execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 1: Enrichment Layer\n",
    "\n",
    "### ðŸ“… `orders_enriched` - Order Header Enrichment\n",
    "\n",
    "Enriches raw order data with:\n",
    "- **Temporal dimensions:** date, day of week, hour\n",
    "- **Numeric conversions:** discount amounts\n",
    "- **Derived flags:** has_discount indicator\n",
    "\n",
    "**Target Lag:** 12 hours (refreshes when data is >12 hours stale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Implementation (Snowpark DataFrame API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 1 dynamic table 'orders_enriched' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build orders_enriched Dynamic Table query using Snowpark DataFrame API\n",
    "orders_enriched_df = session.table(\"tasty_bytes_db.raw.order_header\") \\\n",
    "    .select(\n",
    "        # Order identifiers\n",
    "        F.col(\"order_id\"),\n",
    "        F.col(\"truck_id\"),\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"order_channel\"),\n",
    "        # Temporal dimensions\n",
    "        F.col(\"order_ts\").alias(\"order_timestamp\"),\n",
    "        F.to_date(F.col(\"order_ts\")).alias(\"order_date\"),\n",
    "        F.call_builtin(\"DAYNAME\", F.col(\"order_ts\")).alias(\"day_name\"),\n",
    "        F.hour(F.col(\"order_ts\")).alias(\"order_hour\"),\n",
    "        # Financial metrics\n",
    "        F.col(\"order_amount\"),\n",
    "        F.col(\"order_total\"),\n",
    "        F.call_builtin(\"TRY_TO_NUMBER\", F.col(\"order_discount_amount\"), F.lit(10), F.lit(2)).alias(\"order_discount_amount\"),\n",
    "        # Simple discount flag\n",
    "        F.when(\n",
    "            (F.col(\"discount_id\").isNotNull()) & (F.col(\"discount_id\") != \"\"),\n",
    "            F.lit(True)\n",
    "        ).otherwise(F.lit(False)).alias(\"has_discount\")\n",
    "    ) \\\n",
    "    .where(\n",
    "        F.col(\"order_id\").isNotNull() & F.col(\"order_ts\").isNotNull()\n",
    "    )\n",
    "\n",
    "# Create the dynamic table\n",
    "orders_enriched_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.orders_enriched\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"12 hours\"\n",
    ")\n",
    "\n",
    "print(\"Tier 1 dynamic table 'orders_enriched' created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Implementation (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create orders_enriched dynamic table query\n",
    "# orders_enriched_query = \"\"\"\n",
    "# SELECT\n",
    "#     -- Order identifiers\n",
    "#     order_id,\n",
    "#     truck_id,\n",
    "#     customer_id,\n",
    "#     order_channel,\n",
    "#     -- Temporal dimensions\n",
    "#     order_ts AS order_timestamp,\n",
    "#     DATE(order_ts) AS order_date,\n",
    "#     DAYNAME(order_ts) AS day_name,\n",
    "#     HOUR(order_ts) AS order_hour,\n",
    "#     -- Financial metrics\n",
    "#     order_amount,\n",
    "#     order_total,\n",
    "#     TRY_TO_NUMBER(order_discount_amount, 10, 2) AS order_discount_amount,\n",
    "#     -- Simple discount flag\n",
    "#     CASE\n",
    "#         WHEN discount_id IS NOT NULL AND discount_id != '' THEN TRUE\n",
    "#         ELSE FALSE\n",
    "#     END AS has_discount\n",
    "# FROM tasty_bytes_db.raw.order_header\n",
    "# WHERE order_id IS NOT NULL\n",
    "#     AND order_ts IS NOT NULL\n",
    "# \"\"\"\n",
    "\n",
    "# orders_enriched_dt = DynamicTable(\n",
    "#     name=\"orders_enriched_sql\",\n",
    "#     target_lag=UserDefinedLag(seconds=43200),  # 12 hours\n",
    "#     warehouse=\"PYDATA_LAB_WH\",\n",
    "#     query=orders_enriched_query\n",
    "# )\n",
    "\n",
    "# root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "#     orders_enriched_dt,\n",
    "#     mode=CreateMode.or_replace\n",
    "# )\n",
    "\n",
    "# print(\"Tier 1 dynamic table 'orders_enriched_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ›’ `order_items_enriched` - Line Item Enrichment\n",
    "\n",
    "Joins order details with menu to add:\n",
    "- **Product information:** name, category, brand\n",
    "- **Profit metrics:** unit profit, line profit, margin %\n",
    "- **Discount tracking:** line-level discounts\n",
    "\n",
    "**Target Lag:** 12 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Implementation (Snowpark DataFrame API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 1 dynamic table 'order_items_enriched' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build order_items_enriched using Snowpark DataFrame API\n",
    "od = session.table(\"tasty_bytes_db.raw.order_detail\")\n",
    "m = session.table(\"tasty_bytes_db.raw.menu\")\n",
    "\n",
    "order_items_enriched_df = od.join(m, od[\"menu_item_id\"] == m[\"menu_item_id\"], \"inner\") \\\n",
    "    .select(\n",
    "        # Order detail identifiers\n",
    "        od[\"order_detail_id\"],\n",
    "        od[\"order_id\"],\n",
    "        od[\"line_number\"],\n",
    "        # Product information\n",
    "        od[\"menu_item_id\"].alias(\"menu_item_id\"),\n",
    "        m[\"menu_item_name\"],\n",
    "        m[\"item_category\"],\n",
    "        m[\"item_subcategory\"],\n",
    "        m[\"truck_brand_name\"],\n",
    "        m[\"menu_type\"],\n",
    "        # Quantity and pricing\n",
    "        od[\"quantity\"],\n",
    "        od[\"unit_price\"],\n",
    "        od[\"price\"].alias(\"line_total\"),\n",
    "        m[\"cost_of_goods_usd\"],\n",
    "        m[\"sale_price_usd\"],\n",
    "        # Profit calculations\n",
    "        (od[\"unit_price\"] - m[\"cost_of_goods_usd\"]).alias(\"unit_profit\"),\n",
    "        ((od[\"unit_price\"] - m[\"cost_of_goods_usd\"]) * od[\"quantity\"]).alias(\"line_profit\"),\n",
    "        F.when(\n",
    "            od[\"unit_price\"] > 0,\n",
    "            F.round(((od[\"unit_price\"] - m[\"cost_of_goods_usd\"]) / od[\"unit_price\"]) * 100, 2)\n",
    "        ).otherwise(0).alias(\"profit_margin_pct\"),\n",
    "        # Discount information\n",
    "        F.call_builtin(\"TRY_TO_NUMBER\", od[\"order_item_discount_amount\"], F.lit(10), F.lit(2)).alias(\"line_discount_amount\"),\n",
    "        F.when(\n",
    "            (od[\"discount_id\"].isNotNull()) & (od[\"discount_id\"] != \"\"),\n",
    "            F.lit(True)\n",
    "        ).otherwise(F.lit(False)).alias(\"has_discount\")\n",
    "    ) \\\n",
    "    .where(\n",
    "        od[\"order_id\"].isNotNull() & od[\"menu_item_id\"].isNotNull()\n",
    "    )\n",
    "\n",
    "# Create the dynamic table\n",
    "order_items_enriched_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.order_items_enriched\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"12 hours\"\n",
    ")\n",
    "\n",
    "print(\"Tier 1 dynamic table 'order_items_enriched' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Implementation (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create order_items_enriched dynamic table\n",
    "# order_items_enriched_query = \"\"\"\n",
    "# SELECT\n",
    "#     -- Order detail identifiers\n",
    "#     od.order_detail_id,\n",
    "#     od.order_id,\n",
    "#     od.line_number,\n",
    "#     -- Product information\n",
    "#     od.menu_item_id,\n",
    "#     m.menu_item_name,\n",
    "#     m.item_category,\n",
    "#     m.item_subcategory,\n",
    "#     m.truck_brand_name,\n",
    "#     m.menu_type,\n",
    "#     -- Quantity and pricing\n",
    "#     od.quantity,\n",
    "#     od.unit_price,\n",
    "#     od.price AS line_total,\n",
    "#     m.cost_of_goods_usd,\n",
    "#     m.sale_price_usd,\n",
    "#     -- Profit calculations\n",
    "#     (od.unit_price - m.cost_of_goods_usd) AS unit_profit,\n",
    "#     (od.unit_price - m.cost_of_goods_usd) * od.quantity AS line_profit,\n",
    "#     CASE\n",
    "#         WHEN od.unit_price > 0 THEN\n",
    "#             ROUND(((od.unit_price - m.cost_of_goods_usd) / od.unit_price) * 100, 2)\n",
    "#         ELSE 0\n",
    "#     END AS profit_margin_pct,\n",
    "#     -- Discount information\n",
    "#     TRY_TO_NUMBER(od.order_item_discount_amount, 10, 2) AS line_discount_amount,\n",
    "#     CASE\n",
    "#         WHEN od.discount_id IS NOT NULL AND od.discount_id != '' THEN TRUE\n",
    "#         ELSE FALSE\n",
    "#     END AS has_discount\n",
    "# FROM tasty_bytes_db.raw.order_detail od\n",
    "# INNER JOIN tasty_bytes_db.raw.menu m\n",
    "#     ON od.menu_item_id = m.menu_item_id\n",
    "# WHERE od.order_id IS NOT NULL\n",
    "#     AND od.menu_item_id IS NOT NULL\n",
    "# \"\"\"\n",
    "\n",
    "# order_items_enriched_dt = DynamicTable(\n",
    "#     name=\"order_items_enriched_sql\",\n",
    "#     target_lag=UserDefinedLag(seconds=43200),  # 12 hours\n",
    "#     warehouse=\"PYDATA_LAB_WH\",\n",
    "#     query=order_items_enriched_query\n",
    "# )\n",
    "\n",
    "# root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "#     order_items_enriched_dt,\n",
    "#     mode=CreateMode.or_replace\n",
    "# )\n",
    "\n",
    "# print(\"Tier 1 dynamic table 'order_items_enriched_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Tier 1 Dynamic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from orders_enriched:\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"ORDER_DATE\"  |\"DAY_NAME\"  |\"ORDER_HOUR\"  |\"ORDER_AMOUNT\"  |\"ORDER_TOTAL\"  |\"HAS_DISCOUNT\"  |\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "|78058528    |2022-08-14    |Sun         |20            |19.0000         |19.0000        |False           |\n",
      "|78058529    |2022-08-14    |Sun         |20            |11.0000         |11.0000        |False           |\n",
      "|78058530    |2022-08-14    |Sun         |20            |106.0000        |106.0000       |False           |\n",
      "|78058531    |2022-08-14    |Sun         |20            |26.0000         |26.0000        |False           |\n",
      "|78058532    |2022-08-14    |Sun         |20            |60.0000         |60.0000        |False           |\n",
      "|78058533    |2022-08-14    |Sun         |20            |24.0000         |24.0000        |False           |\n",
      "|78058534    |2022-08-14    |Sun         |20            |24.0000         |24.0000        |False           |\n",
      "|78058535    |2022-08-14    |Sun         |20            |69.0000         |69.0000        |False           |\n",
      "|78058536    |2022-08-14    |Sun         |20            |37.0000         |37.0000        |False           |\n",
      "|78058537    |2022-08-14    |Sun         |20            |116.0000        |116.0000       |False           |\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check orders_enriched using Snowpark\n",
    "print(\"Sample data from orders_enriched:\")\n",
    "\n",
    "# Replace with orders_enriched_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.orders_enriched\") \\\n",
    "    .select(\"order_id\", \"order_date\", \"day_name\", \"order_hour\", \"order_amount\", \"order_total\", \"has_discount\") \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from order_items_enriched:\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_NAME\"            |\"ITEM_CATEGORY\"  |\"QUANTITY\"  |\"UNIT_PRICE\"  |\"LINE_TOTAL\"  |\"LINE_PROFIT\"  |\"PROFIT_MARGIN_PCT\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|Coney Dog                   |Main             |1           |10.0000       |10.0000       |5.0000         |50.00                |\n",
      "|Breakfast Crepe             |Main             |1           |12.0000       |12.0000       |7.0000         |58.33                |\n",
      "|The Classic                 |Main             |3           |12.0000       |36.0000       |24.0000        |66.67                |\n",
      "|Lean Beef Tibs              |Main             |3           |13.0000       |39.0000       |21.0000        |53.85                |\n",
      "|Bottled Soda                |Beverage         |1           |3.0000        |3.0000        |2.5000         |83.33                |\n",
      "|Spicy Miso Vegetable Ramen  |Main             |1           |17.2500       |17.2500       |10.2500        |59.42                |\n",
      "|Bottled Water               |Beverage         |1           |2.0000        |2.0000        |1.5000         |75.00                |\n",
      "|Bottled Water               |Beverage         |1           |2.0000        |2.0000        |1.5000         |75.00                |\n",
      "|The Salad of All Salads     |Main             |1           |12.0000       |12.0000       |6.0000         |50.00                |\n",
      "|Crepe Suzette               |Snack            |1           |9.0000        |9.0000        |5.0000         |55.56                |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check order_items_enriched using Snowpark\n",
    "print(\"Sample data from order_items_enriched:\")\n",
    "\n",
    "# Replace with orders_items_enriched_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.order_items_enriched\") \\\n",
    "    .select(\"menu_item_name\", \"item_category\", \"quantity\", \"unit_price\", \"line_total\", \"line_profit\", \"profit_margin_pct\") \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 2: Fact Table\n",
    "\n",
    "### ðŸ“‹ `order_fact` - Unified Order View\n",
    "\n",
    "Joins Tier 1 tables into a comprehensive fact table containing all order and line item details.\n",
    "\n",
    "**Target Lag:** `DOWNSTREAM` (auto-refreshes when upstream Tier 1 tables refresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Implementation (Snowpark DataFrame API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 2 Dynamic table 'order_fact' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build order_fact query using Snowpark DataFrame API\n",
    "o = session.table(\"tasty_bytes_db.analytics.orders_enriched\")\n",
    "oi = session.table(\"tasty_bytes_db.analytics.order_items_enriched\")\n",
    "\n",
    "order_fact_df = o.join(oi, o[\"order_id\"] == oi[\"order_id\"], \"inner\") \\\n",
    "    .select(\n",
    "        # Order header fields\n",
    "        o[\"order_id\"].alias(\"order_id\"),\n",
    "        o[\"truck_id\"],\n",
    "        o[\"customer_id\"],\n",
    "        o[\"order_channel\"],\n",
    "        o[\"order_timestamp\"],\n",
    "        o[\"order_date\"],\n",
    "        o[\"day_name\"],\n",
    "        o[\"order_hour\"],\n",
    "        o[\"order_amount\"],\n",
    "        o[\"order_total\"],\n",
    "        o[\"order_discount_amount\"].alias(\"order_level_discount\"),\n",
    "        o[\"has_discount\"].alias(\"order_has_discount\"),\n",
    "        # Order line item fields\n",
    "        oi[\"order_detail_id\"],\n",
    "        oi[\"line_number\"],\n",
    "        oi[\"menu_item_id\"],\n",
    "        oi[\"menu_item_name\"],\n",
    "        oi[\"item_category\"],\n",
    "        oi[\"item_subcategory\"],\n",
    "        oi[\"truck_brand_name\"],\n",
    "        oi[\"menu_type\"],\n",
    "        oi[\"quantity\"],\n",
    "        oi[\"unit_price\"],\n",
    "        oi[\"line_total\"],\n",
    "        oi[\"cost_of_goods_usd\"],\n",
    "        oi[\"sale_price_usd\"],\n",
    "        oi[\"unit_profit\"],\n",
    "        oi[\"line_profit\"],\n",
    "        oi[\"profit_margin_pct\"],\n",
    "        oi[\"line_discount_amount\"],\n",
    "        oi[\"has_discount\"].alias(\"line_has_discount\")\n",
    "    )\n",
    "\n",
    "# Create the dynamic table\n",
    "order_fact_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.order_fact\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"DOWNSTREAM\"\n",
    ")\n",
    "\n",
    "print(\"Tier 2 Dynamic table 'order_fact' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Implementation (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create order_fact dynamic table query\n",
    "# order_fact_query = \"\"\"\n",
    "# SELECT\n",
    "#     -- Order header fields\n",
    "#     o.order_id,\n",
    "#     o.truck_id,\n",
    "#     o.customer_id,\n",
    "#     o.order_channel,\n",
    "#     o.order_timestamp,\n",
    "#     o.order_date,\n",
    "#     o.day_name,\n",
    "#     o.order_hour,\n",
    "#     o.order_amount,\n",
    "#     o.order_total,\n",
    "#     o.order_discount_amount AS order_level_discount,\n",
    "#     o.has_discount AS order_has_discount,\n",
    "#     -- Order line item fields\n",
    "#     oi.order_detail_id,\n",
    "#     oi.line_number,\n",
    "#     oi.menu_item_id,\n",
    "#     oi.menu_item_name,\n",
    "#     oi.item_category,\n",
    "#     oi.item_subcategory,\n",
    "#     oi.truck_brand_name,\n",
    "#     oi.menu_type,\n",
    "#     oi.quantity,\n",
    "#     oi.unit_price,\n",
    "#     oi.line_total,\n",
    "#     oi.cost_of_goods_usd,\n",
    "#     oi.sale_price_usd,\n",
    "#     oi.unit_profit,\n",
    "#     oi.line_profit,\n",
    "#     oi.profit_margin_pct,\n",
    "#     oi.line_discount_amount,\n",
    "#     oi.has_discount AS line_has_discount\n",
    "# FROM tasty_bytes_db.analytics.orders_enriched_sql o\n",
    "# INNER JOIN tasty_bytes_db.analytics.order_items_enriched_sql oi\n",
    "#     ON o.order_id = oi.order_id\n",
    "# \"\"\"\n",
    "\n",
    "# order_fact_dt = DynamicTable(\n",
    "#     name=\"order_fact_sql\",\n",
    "#     target_lag=DownstreamLag(),  # Refresh when upstream tables refresh\n",
    "#     warehouse=\"PYDATA_LAB_WH\",\n",
    "#     query=order_fact_query\n",
    "# )\n",
    "\n",
    "# root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "#     order_fact_dt,\n",
    "#     mode=CreateMode.or_replace\n",
    "# )\n",
    "\n",
    "# print(\"Tier 2 Dynamic table 'order_fact_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Tier 2 Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from order_fact:\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"ORDER_DATE\"  |\"MENU_ITEM_NAME\"           |\"ITEM_CATEGORY\"  |\"QUANTITY\"  |\"ORDER_TOTAL\"  |\"LINE_PROFIT\"  |\"PROFIT_MARGIN_PCT\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|395274135   |2022-04-20    |Lobster Mac & Cheese       |Main             |3           |94.0000        |15.0000        |33.33                |\n",
      "|113376148   |2022-01-01    |Bottled Water              |Beverage         |1           |28.0000        |1.5000         |75.00                |\n",
      "|446638561   |2022-08-02    |Coney Dog                  |Main             |2           |29.0000        |10.0000        |50.00                |\n",
      "|34408081    |2021-03-03    |Hot Ham & Cheese           |Main             |3           |47.0000        |12.0000        |36.36                |\n",
      "|408864784   |2020-04-21    |Coney Dog                  |Main             |1           |10.0000        |5.0000         |50.00                |\n",
      "|63463760    |2022-09-28    |Lean Chicken Tikka Masala  |Main             |1           |56.0000        |7.0000         |41.18                |\n",
      "|97814440    |2021-12-13    |Lobster Mac & Cheese       |Main             |3           |99.0000        |15.0000        |33.33                |\n",
      "|32845061    |2022-10-30    |Breakfast Crepe            |Main             |1           |12.0000        |7.0000         |58.33                |\n",
      "|401551175   |2021-10-06    |Three Meat Plate           |Main             |1           |51.0000        |7.0000         |41.18                |\n",
      "|393799490   |2021-07-15    |Chicken Burrito            |Main             |1           |58.0000        |9.2500         |74.00                |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check order_fact using Snowpark\n",
    "print(\"Sample data from order_fact:\")\n",
    "\n",
    "# Replace with order_fact_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.order_fact\") \\\n",
    "    .select(\"order_id\", \"order_date\", \"menu_item_name\", \"item_category\", \"quantity\", \"order_total\", \"line_profit\", \"profit_margin_pct\") \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier 3: Aggregated Metrics\n",
    "\n",
    "### ðŸ“ˆ `daily_business_metrics` - Daily KPIs\n",
    "\n",
    "Pre-aggregates key business metrics by day:\n",
    "\n",
    "| Metric Category | Examples |\n",
    "|-----------------|----------|\n",
    "| **Volume** | Orders, trucks, customers, items sold |\n",
    "| **Revenue** | Total revenue, average order value |\n",
    "| **Profit** | Total profit, average margin |\n",
    "| **Discounts** | Orders with discounts, discount amounts |\n",
    "\n",
    "**Target Lag:** `DOWNSTREAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tier 3) Dynamic table 'daily_business_metrics' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build daily_business_metrics using Snowpark DataFrame API\n",
    "daily_business_metrics_df = session.table(\"tasty_bytes_db.analytics.order_fact\") \\\n",
    "    .group_by(\"order_date\", \"day_name\") \\\n",
    "    .agg(\n",
    "        # Volume metrics\n",
    "        F.count_distinct(F.col(\"order_id\")).alias(\"total_orders\"),\n",
    "        F.count_distinct(F.col(\"truck_id\")).alias(\"active_trucks\"),\n",
    "        F.count_distinct(F.col(\"customer_id\")).alias(\"unique_customers\"),\n",
    "        F.sum(F.col(\"quantity\")).alias(\"total_items_sold\"),\n",
    "        # Revenue metrics\n",
    "        F.sum(F.col(\"order_total\")).alias(\"total_revenue\"),\n",
    "        F.round(F.avg(F.col(\"order_total\")), 2).alias(\"avg_order_value\"),\n",
    "        F.sum(F.col(\"line_total\")).alias(\"total_line_item_revenue\"),\n",
    "        # Profit metrics\n",
    "        F.sum(F.col(\"line_profit\")).alias(\"total_profit\"),\n",
    "        F.round(F.avg(F.col(\"profit_margin_pct\")), 2).alias(\"avg_profit_margin_pct\"),\n",
    "        # Discount metrics\n",
    "        F.sum(F.when(F.col(\"order_has_discount\"), 1).otherwise(0)).alias(\"orders_with_discount\"),\n",
    "        F.sum(F.col(\"order_level_discount\")).alias(\"total_order_discount_amount\"),\n",
    "        F.sum(F.col(\"line_discount_amount\")).alias(\"total_line_discount_amount\")\n",
    "    )\n",
    "\n",
    "# Create the dynamic table using Snowpark DataFrame method\n",
    "daily_business_metrics_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.daily_business_metrics\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"DOWNSTREAM\"\n",
    ")\n",
    "\n",
    "print(\"(Tier 3) Dynamic table 'daily_business_metrics' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Implementation (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create daily_business_metrics dynamic table\n",
    "# daily_business_metrics_query = \"\"\"\n",
    "# SELECT\n",
    "#     order_date,\n",
    "#     day_name,\n",
    "#     -- Volume metrics\n",
    "#     COUNT(DISTINCT order_id) AS total_orders,\n",
    "#     COUNT(DISTINCT truck_id) AS active_trucks,\n",
    "#     COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "#     SUM(quantity) AS total_items_sold,\n",
    "#     -- Revenue metrics\n",
    "#     SUM(order_total) AS total_revenue,\n",
    "#     ROUND(AVG(order_total), 2) AS avg_order_value,\n",
    "#     SUM(line_total) AS total_line_item_revenue,\n",
    "#     -- Profit metrics\n",
    "#     SUM(line_profit) AS total_profit,\n",
    "#     ROUND(AVG(profit_margin_pct), 2) AS avg_profit_margin_pct,\n",
    "#     -- Discount metrics\n",
    "#     SUM(CASE WHEN order_has_discount THEN 1 ELSE 0 END) AS orders_with_discount,\n",
    "#     SUM(order_level_discount) AS total_order_discount_amount,\n",
    "#     SUM(line_discount_amount) AS total_line_discount_amount\n",
    "# FROM tasty_bytes_db.analytics.order_fact_sql\n",
    "# GROUP BY order_date, day_name\n",
    "# \"\"\"\n",
    "\n",
    "# daily_business_metrics_dt = DynamicTable(\n",
    "#     name=\"daily_business_metrics_sql\",\n",
    "#     target_lag=DownstreamLag(),  # Refresh when upstream tables refresh\n",
    "#     warehouse=\"PYDATA_LAB_WH\",\n",
    "#     query=daily_business_metrics_query\n",
    "# )\n",
    "\n",
    "# root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "#     daily_business_metrics_dt,\n",
    "#     mode=CreateMode.or_replace\n",
    "# )\n",
    "\n",
    "# print(\"(Tier 3) Dynamic table 'daily_business_metrics_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ† `product_performance_metrics` - Product Analytics\n",
    "\n",
    "Aggregates sales and profit data by product:\n",
    "\n",
    "| Metric Category | Examples |\n",
    "|-----------------|----------|\n",
    "| **Dimensions** | Name, category, subcategory, brand |\n",
    "| **Volume** | Order count, units sold |\n",
    "| **Financial** | Revenue, profit, margins |\n",
    "| **Performance** | Revenue per unit, profit per unit |\n",
    "\n",
    "**Target Lag:** `DOWNSTREAM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tier 3) Dynamic table 'product_performance_metrics created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build product_performance_metrics using Snowpark DataFrame API\n",
    "product_performance_metrics_df = session.table(\"tasty_bytes_db.analytics.order_fact\") \\\n",
    "    .group_by(\n",
    "        \"menu_item_id\",\n",
    "        \"menu_item_name\",\n",
    "        \"item_category\",\n",
    "        \"item_subcategory\",\n",
    "        \"truck_brand_name\",\n",
    "        \"menu_type\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        # Sales volume metrics\n",
    "        F.count_distinct(F.col(\"order_id\")).alias(\"order_count\"),\n",
    "        F.sum(F.col(\"quantity\")).alias(\"total_units_sold\"),\n",
    "        # Revenue and profit metrics\n",
    "        F.sum(F.col(\"line_total\")).alias(\"total_revenue\"),\n",
    "        F.sum(F.col(\"line_profit\")).alias(\"total_profit\"),\n",
    "        F.round(F.avg(F.col(\"unit_price\")), 2).alias(\"avg_unit_price\"),\n",
    "        F.round(F.avg(F.col(\"profit_margin_pct\")), 2).alias(\"avg_profit_margin_pct\"),\n",
    "        # Cost metrics\n",
    "        F.avg(F.col(\"cost_of_goods_usd\")).alias(\"avg_cogs\"),\n",
    "        F.avg(F.col(\"sale_price_usd\")).alias(\"standard_sale_price\"),\n",
    "        # Performance indicators\n",
    "        (F.sum(F.col(\"line_total\")) / F.call_builtin(\"NULLIF\", F.sum(F.col(\"quantity\")), F.lit(0))).alias(\"revenue_per_unit\"),\n",
    "        (F.sum(F.col(\"line_profit\")) / F.call_builtin(\"NULLIF\", F.sum(F.col(\"quantity\")), F.lit(0))).alias(\"profit_per_unit\")\n",
    "    )\n",
    "\n",
    "# Create the dynamic table using Snowpark DataFrame method\n",
    "product_performance_metrics_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.product_performance_metrics\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"DOWNSTREAM\"\n",
    ")\n",
    "\n",
    "print(\"(Tier 3) Dynamic table 'product_performance_metrics created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Implementation (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create product_performance_metrics dynamic table\n",
    "# product_performance_metrics_query = \"\"\"\n",
    "# SELECT\n",
    "#     -- Product dimensions\n",
    "#     menu_item_id,\n",
    "#     menu_item_name,\n",
    "#     item_category,\n",
    "#     item_subcategory,\n",
    "#     truck_brand_name,\n",
    "#     menu_type,\n",
    "#     -- Sales volume metrics\n",
    "#     COUNT(DISTINCT order_id) AS order_count,\n",
    "#     SUM(quantity) AS total_units_sold,\n",
    "#     -- Revenue and profit metrics\n",
    "#     SUM(line_total) AS total_revenue,\n",
    "#     SUM(line_profit) AS total_profit,\n",
    "#     ROUND(AVG(unit_price), 2) AS avg_unit_price,\n",
    "#     ROUND(AVG(profit_margin_pct), 2) AS avg_profit_margin_pct,\n",
    "#     -- Cost metrics\n",
    "#     AVG(cost_of_goods_usd) AS avg_cogs,\n",
    "#     AVG(sale_price_usd) AS standard_sale_price,\n",
    "#     -- Performance indicators\n",
    "#     SUM(line_total) / NULLIF(SUM(quantity), 0) AS revenue_per_unit,\n",
    "#     SUM(line_profit) / NULLIF(SUM(quantity), 0) AS profit_per_unit\n",
    "# FROM tasty_bytes_db.analytics.order_fact_sql\n",
    "# GROUP BY\n",
    "#     menu_item_id,\n",
    "#     menu_item_name,\n",
    "#     item_category,\n",
    "#     item_subcategory,\n",
    "#     truck_brand_name,\n",
    "#     menu_type\n",
    "# \"\"\"\n",
    "\n",
    "# product_performance_metrics_dt = DynamicTable(\n",
    "#     name=\"product_performance_metrics_sql\",\n",
    "#     target_lag=DownstreamLag(),  # Refresh when upstream tables refresh\n",
    "#     warehouse=\"PYDATA_LAB_WH\",\n",
    "#     query=product_performance_metrics_query\n",
    "# )\n",
    "\n",
    "# root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "#     product_performance_metrics_dt,\n",
    "#     mode=CreateMode.or_replace\n",
    "# )\n",
    "\n",
    "# print(\"(Tier 3) Dynamic table 'product_performance_metrics_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Tier 3 Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from daily_business_metrics:\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_DATE\"  |\"DAY_NAME\"  |\"TOTAL_ORDERS\"  |\"UNIQUE_CUSTOMERS\"  |\"TOTAL_REVENUE\"  |\"AVG_ORDER_VALUE\"  |\"TOTAL_PROFIT\"  |\"AVG_PROFIT_MARGIN_PCT\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2022-11-01    |Tue         |425886          |18644               |59793523.2500    |51.75              |8961352.6000    |58.66                    |\n",
      "|2022-10-31    |Mon         |412664          |15854               |58254392.2500    |51.90              |8705875.3000    |58.67                    |\n",
      "|2022-10-30    |Sun         |459105          |18020               |64367132.7500    |51.62              |9647234.4500    |58.68                    |\n",
      "|2022-10-29    |Sat         |465857          |18145               |65379979.0000    |51.71              |9797068.6500    |58.67                    |\n",
      "|2022-10-28    |Fri         |448874          |21603               |63159728.0000    |51.81              |9464734.6500    |58.67                    |\n",
      "|2022-10-27    |Thu         |439475          |18029               |61790017.0000    |51.78              |9256160.4500    |58.69                    |\n",
      "|2022-10-26    |Wed         |432436          |16650               |60760310.2500    |51.75              |9102695.2000    |58.69                    |\n",
      "|2022-10-25    |Tue         |431288          |20235               |60639247.2500    |51.76              |9080964.4000    |58.66                    |\n",
      "|2022-10-24    |Mon         |407548          |18816               |57434597.2500    |51.82              |8597204.9500    |58.68                    |\n",
      "|2022-10-23    |Sun         |454339          |15029               |63829794.0000    |51.74              |9563098.8000    |58.67                    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check daily_business_metrics using Snowpark\n",
    "print(\"Sample data from daily_business_metrics:\")\n",
    "\n",
    "# Replace with daily_business_metrics_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.daily_business_metrics\") \\\n",
    "    .select(\"order_date\", \"day_name\", \"total_orders\", \"unique_customers\", \"total_revenue\", \"avg_order_value\", \"total_profit\", \"avg_profit_margin_pct\") \\\n",
    "    .order_by(F.col(\"order_date\").desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from product_performance_metrics (top products by revenue):\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_NAME\"            |\"ITEM_CATEGORY\"  |\"ORDER_COUNT\"  |\"TOTAL_UNITS_SOLD\"  |\"TOTAL_REVENUE\"  |\"TOTAL_PROFIT\"  |\"AVG_PROFIT_MARGIN_PCT\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|The King Combo              |Main             |12627298       |21563360            |431267200.0000   |172506880.0000  |40.00                    |\n",
      "|Tandoori Mixed Grill        |Main             |12097473       |20659415            |371869470.0000   |144615905.0000  |38.89                    |\n",
      "|Lean Chicken Tikka Masala   |Main             |12098077       |20660126            |351222142.0000   |144620882.0000  |41.18                    |\n",
      "|Spicy Miso Vegetable Ramen  |Main             |11064556       |18897578            |325983220.5000   |193700174.5000  |59.42                    |\n",
      "|Tonkotsu Ramen              |Main             |11064124       |18895904            |325954344.0000   |193683016.0000  |59.42                    |\n",
      "|Creamy Chicken Ramen        |Main             |11064017       |18894849            |325936145.2500   |174777353.2500  |53.62                    |\n",
      "|Combination Curry           |Main             |12103092       |20663277            |309949155.0000   |123979662.0000  |40.00                    |\n",
      "|Chicken Pot Pie Crepe       |Main             |11811193       |20171467            |302572005.0000   |181543203.0000  |60.00                    |\n",
      "|Lobster Mac & Cheese        |Main             |10426234       |17807243            |267108645.0000   |89036215.0000   |33.33                    |\n",
      "|Gyro Plate                  |Main             |12626831       |21562600            |258751200.0000   |86250400.0000   |33.33                    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check product_performance_metrics using Snowpark\n",
    "print(\"Sample data from product_performance_metrics (top products by revenue):\")\n",
    "\n",
    "# Replace with product_performance_metrics_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .select(\"menu_item_name\", \"item_category\", \"order_count\", \"total_units_sold\", \"total_revenue\", \"total_profit\", \"avg_profit_margin_pct\") \\\n",
    "    .order_by(F.col(\"total_revenue\").desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“‹ List All Dynamic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic tables in 'analytics' schema:\n",
      "DAILY_BUSINESS_METRICS\n",
      "ORDERS_ENRICHED\n",
      "ORDER_FACT\n",
      "ORDER_ITEMS_ENRICHED\n",
      "PRODUCT_PERFORMANCE_METRICS\n"
     ]
    }
   ],
   "source": [
    "# List all dynamic tables in the analytics schema\n",
    "analytics_schema_ref = root.databases[\"tasty_bytes_db\"].schemas[\"analytics\"]\n",
    "dynamic_table_collection = DynamicTableCollection(analytics_schema_ref)\n",
    "dynamic_tables = dynamic_table_collection.iter()\n",
    "print(\"Dynamic tables in 'analytics' schema:\")\n",
    "for dt in dynamic_tables:\n",
    "    print(dt.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Incremental Refresh âš¡\n",
    "\n",
    "Demonstrate how Dynamic Tables efficiently process only **changed data** rather than reprocessing everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”§ Create Demo Data Generator\n",
    "\n",
    "This stored procedure generates synthetic orders to simulate new data arriving in our system.\n",
    "\n",
    "> **Note:** Stored procedure creation currently requires SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stored procedure 'generate_demo_orders' created\n"
     ]
    }
   ],
   "source": [
    "# Create stored procedure to generate demo orders\n",
    "session.sql(\"\"\"\n",
    "    CREATE OR REPLACE PROCEDURE tasty_bytes_db.raw.generate_demo_orders(num_rows INTEGER)\n",
    "    RETURNS STRING\n",
    "    LANGUAGE SQL\n",
    "    AS\n",
    "    $$\n",
    "    DECLARE\n",
    "        orders_before INTEGER;\n",
    "        orders_after INTEGER;\n",
    "        orders_inserted INTEGER;\n",
    "        details_before INTEGER;\n",
    "        details_after INTEGER;\n",
    "        details_inserted INTEGER;\n",
    "    BEGIN\n",
    "        -- Capture counts before insert\n",
    "        SELECT COUNT(*) INTO :orders_before FROM tasty_bytes_db.raw.order_header;\n",
    "        SELECT COUNT(*) INTO :details_before FROM tasty_bytes_db.raw.order_detail;\n",
    "        \n",
    "        -- Create temporary table with new order IDs to maintain referential integrity\n",
    "        CREATE OR REPLACE TEMPORARY TABLE new_orders AS\n",
    "        SELECT\n",
    "            (1000000 + UNIFORM(1, 999999, RANDOM()))::NUMBER(38,0) AS new_order_id,\n",
    "            oh.order_id AS original_order_id,\n",
    "            oh.truck_id,\n",
    "            oh.location_id,\n",
    "            oh.customer_id,\n",
    "            oh.discount_id,\n",
    "            oh.shift_id,\n",
    "            oh.shift_start_time,\n",
    "            oh.shift_end_time,\n",
    "            oh.order_channel,\n",
    "            DATEADD('day', DATEDIFF('day', oh.order_ts, CURRENT_DATE()), oh.order_ts) AS order_ts,\n",
    "            oh.served_ts,\n",
    "            oh.order_currency,\n",
    "            oh.order_amount * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS order_amount,\n",
    "            oh.order_tax_amount,\n",
    "            oh.order_discount_amount,\n",
    "            oh.order_total * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS order_total\n",
    "        FROM tasty_bytes_db.raw.order_header oh\n",
    "        WHERE oh.order_id IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT :num_rows;\n",
    "        \n",
    "        -- Insert synthetic order headers\n",
    "        INSERT INTO tasty_bytes_db.raw.order_header (\n",
    "            order_id, truck_id, location_id, customer_id, discount_id, shift_id,\n",
    "            shift_start_time, shift_end_time, order_channel, order_ts, served_ts,\n",
    "            order_currency, order_amount, order_tax_amount, order_discount_amount,\n",
    "            order_total\n",
    "        )\n",
    "        SELECT\n",
    "            new_order_id, truck_id, location_id, customer_id, discount_id, shift_id,\n",
    "            shift_start_time, shift_end_time, order_channel, order_ts, served_ts,\n",
    "            order_currency, order_amount, order_tax_amount, order_discount_amount,\n",
    "            order_total\n",
    "        FROM new_orders;\n",
    "        \n",
    "        -- Insert corresponding order details (line items)\n",
    "        INSERT INTO tasty_bytes_db.raw.order_detail (\n",
    "            order_detail_id, order_id, menu_item_id, discount_id, line_number,\n",
    "            quantity, unit_price, price, order_item_discount_amount\n",
    "        )\n",
    "        SELECT\n",
    "            (2000000 + UNIFORM(1, 9999999, RANDOM()))::NUMBER(38,0) AS order_detail_id,\n",
    "            no.new_order_id AS order_id,\n",
    "            od.menu_item_id,\n",
    "            od.discount_id,\n",
    "            od.line_number,\n",
    "            od.quantity,\n",
    "            od.unit_price * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS unit_price,\n",
    "            od.price * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS price,\n",
    "            od.order_item_discount_amount\n",
    "        FROM new_orders no\n",
    "        INNER JOIN tasty_bytes_db.raw.order_detail od\n",
    "            ON no.original_order_id = od.order_id;\n",
    "        \n",
    "        -- Capture counts after insert\n",
    "        SELECT COUNT(*) INTO :orders_after FROM tasty_bytes_db.raw.order_header;\n",
    "        SELECT COUNT(*) INTO :details_after FROM tasty_bytes_db.raw.order_detail;\n",
    "        \n",
    "        orders_inserted := :orders_after - :orders_before;\n",
    "        details_inserted := :details_after - :details_before;\n",
    "        \n",
    "        -- Clean up temporary table\n",
    "        DROP TABLE IF EXISTS new_orders;\n",
    "        \n",
    "        RETURN 'Successfully generated ' || orders_inserted::STRING || ' new orders with ' ||\n",
    "               details_inserted::STRING || ' line items. Total orders: ' || orders_after::STRING;\n",
    "    END;\n",
    "    $$\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"âœ… Stored procedure 'generate_demo_orders' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Capture Initial State\n",
    "\n",
    "Before adding new data, record current row counts for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial order_header count: 248,201,269\n",
      "Initial order_detail count: 673,655,465\n",
      "Latest order timestamp: 2022-11-01 22:59:59\n"
     ]
    }
   ],
   "source": [
    "# Get current row counts using Snowpark\n",
    "initial_order_count = session.table(\"tasty_bytes_db.raw.order_header\").count()\n",
    "initial_detail_count = session.table(\"tasty_bytes_db.raw.order_detail\").count()\n",
    "\n",
    "print(f\"Initial order_header count: {initial_order_count:,}\")\n",
    "print(f\"Initial order_detail count: {initial_detail_count:,}\")\n",
    "\n",
    "# Get latest order timestamp using Snowpark\n",
    "latest_order = session.table(\"tasty_bytes_db.raw.order_header\") \\\n",
    "    .select(F.max(\"order_ts\").alias(\"latest_order_ts\")) \\\n",
    "    .collect()[0][0]\n",
    "print(f\"Latest order timestamp: {latest_order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âž• Generate New Demo Orders\n",
    "\n",
    "Call the stored procedure to generate 1,200 new synthetic orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 1200 new orders with 3275 line items. Total orders: 248202469\n"
     ]
    }
   ],
   "source": [
    "# Call stored procedure to generate 1200 new orders\n",
    "result = session.call(\"tasty_bytes_db.raw.generate_demo_orders\", 1200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify New Data Inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New order_header count: 248,202,469 (+1,200 rows)\n",
      "New order_detail count: 673,658,740 (+3,275 rows)\n"
     ]
    }
   ],
   "source": [
    "# Get updated row counts\n",
    "new_order_count = session.table(\"tasty_bytes_db.raw.order_header\").count()\n",
    "new_detail_count = session.table(\"tasty_bytes_db.raw.order_detail\").count()\n",
    "\n",
    "print(f\"New order_header count: {new_order_count:,} (+{new_order_count - initial_order_count:,} rows)\")\n",
    "print(f\"New order_detail count: {new_detail_count:,} (+{new_detail_count - initial_detail_count:,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Trigger Tier 1 Refresh\n",
    "\n",
    "Manually refresh Tier 1 tables to process the new data incrementally.\n",
    "\n",
    "> **Incremental Refresh:** Only the new/changed rows are processedâ€”much faster than a full refresh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing orders_enriched...\n",
      "Refreshing order_items_enriched...\n",
      "Tier 1 tables refreshed\n"
     ]
    }
   ],
   "source": [
    "# Refresh Tier 1 tables\n",
    "# Append _sql if you want to refresh those created with SQL queries\n",
    "\n",
    "print(\"Refreshing orders_enriched...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.orders_enriched REFRESH\").collect()\n",
    "\n",
    "print(\"Refreshing order_items_enriched...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.order_items_enriched REFRESH\").collect()\n",
    "\n",
    "print(\"Tier 1 tables refreshed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“œ Check Tier 1 Refresh History\n",
    "\n",
    "Query refresh history to see if Snowflake performed `INCREMENTAL` or `FULL` refresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh history for orders_enriched:\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"           |\"REFRESH_ACTION\"  |\"STATE\"    |\"REFRESH_START_TIME\"              |\"REFRESH_TRIGGER\"  |\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "|ORDERS_ENRICHED  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:28.435000-08:00  |MANUAL             |\n",
      "|ORDERS_ENRICHED  |NO_DATA           |SUCCEEDED  |2025-12-24 13:27:06.249000-08:00  |CREATION           |\n",
      "|ORDERS_ENRICHED  |NO_DATA           |SUCCEEDED  |2025-12-24 13:26:58.862000-08:00  |CREATION           |\n",
      "|ORDERS_ENRICHED  |NO_DATA           |SUCCEEDED  |2025-12-24 13:25:08.103000-08:00  |CREATION           |\n",
      "|ORDERS_ENRICHED  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:23:27.600000-08:00  |CREATION           |\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check refresh history for orders_enriched\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for orders_enriched:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time, refresh_trigger\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.ORDERS_ENRICHED'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh history for order_items_enriched:\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"                |\"REFRESH_ACTION\"  |\"STATE\"    |\"REFRESH_START_TIME\"              |\"REFRESH_TRIGGER\"  |\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "|ORDER_ITEMS_ENRICHED  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:29.523000-08:00  |MANUAL             |\n",
      "|ORDER_ITEMS_ENRICHED  |NO_DATA           |SUCCEEDED  |2025-12-24 13:27:06.255000-08:00  |CREATION           |\n",
      "|ORDER_ITEMS_ENRICHED  |NO_DATA           |SUCCEEDED  |2025-12-24 13:26:58.862000-08:00  |CREATION           |\n",
      "|ORDER_ITEMS_ENRICHED  |NO_DATA           |SUCCEEDED  |2025-12-24 13:25:08.096000-08:00  |CREATION           |\n",
      "|ORDER_ITEMS_ENRICHED  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:23:43.841000-08:00  |CREATION           |\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check refresh history for order_items_enriched\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for order_items_enriched:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time, refresh_trigger\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.ORDER_ITEMS_ENRICHED'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Trigger Tier 2 & 3 Refresh\n",
    "\n",
    "Refresh downstream tables (they use `DOWNSTREAM` lag, so they cascade automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refreshing order_fact...\n",
      "Refreshing daily_business_metrics...\n",
      "Refreshing product_performance_metrics...\n",
      "All downstream tables refreshed\n"
     ]
    }
   ],
   "source": [
    "# Refresh Tier 2 table\n",
    "# Append _sql if you want to refresh those created with SQL queries\n",
    "\n",
    "print(\"Refreshing order_fact...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.order_fact REFRESH\").collect()\n",
    "\n",
    "# Refresh Tier 3 tables\n",
    "print(\"Refreshing daily_business_metrics...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.daily_business_metrics REFRESH\").collect()\n",
    "\n",
    "print(\"Refreshing product_performance_metrics...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.product_performance_metrics REFRESH\").collect()\n",
    "\n",
    "print(\"All downstream tables refreshed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“œ Check Tier 2 & 3 Refresh History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh history for order_fact:\n",
      "--------------------------------------------------------------------------------\n",
      "|\"NAME\"      |\"REFRESH_ACTION\"  |\"STATE\"    |\"REFRESH_START_TIME\"              |\n",
      "--------------------------------------------------------------------------------\n",
      "|ORDER_FACT  |NO_DATA           |SUCCEEDED  |2025-12-24 13:34:45.008000-08:00  |\n",
      "|ORDER_FACT  |NO_DATA           |SUCCEEDED  |2025-12-24 13:34:41.840000-08:00  |\n",
      "|ORDER_FACT  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:39.127000-08:00  |\n",
      "|ORDER_FACT  |NO_DATA           |SUCCEEDED  |2025-12-24 13:27:06.729000-08:00  |\n",
      "|ORDER_FACT  |NO_DATA           |SUCCEEDED  |2025-12-24 13:26:59.289000-08:00  |\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check refresh history for order_fact\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for order_fact:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.ORDER_FACT'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh history for daily_business_metrics:\n",
      "--------------------------------------------------------------------------------------------\n",
      "|\"NAME\"                  |\"REFRESH_ACTION\"  |\"STATE\"    |\"REFRESH_START_TIME\"              |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|DAILY_BUSINESS_METRICS  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:42.306000-08:00  |\n",
      "|DAILY_BUSINESS_METRICS  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:26:59.681000-08:00  |\n",
      "--------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check refresh history for daily_business_metrics\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for daily_business_metrics:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.DAILY_BUSINESS_METRICS'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh history for product_performance_metrics:\n",
      "-------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"                       |\"REFRESH_ACTION\"  |\"STATE\"    |\"REFRESH_START_TIME\"              |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "|PRODUCT_PERFORMANCE_METRICS  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:45.721000-08:00  |\n",
      "|PRODUCT_PERFORMANCE_METRICS  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:27:07.157000-08:00  |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check refresh history for product_performance_metrics\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for product_performance_metrics:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.PRODUCT_PERFORMANCE_METRICS'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Updated Metrics\n",
    "\n",
    "Confirm that new data has propagated through the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated daily business metrics (most recent dates):\n",
      "-----------------------------------------------------------------------------------------\n",
      "|\"ORDER_DATE\"  |\"TOTAL_ORDERS\"  |\"TOTAL_ITEMS_SOLD\"  |\"TOTAL_REVENUE\"  |\"TOTAL_PROFIT\"  |\n",
      "-----------------------------------------------------------------------------------------\n",
      "|2025-12-24    |1200            |4991                |171300.4000      |25864.4000      |\n",
      "|2022-11-01    |425886          |1765938             |59793523.2500    |8961352.6000    |\n",
      "|2022-10-31    |412664          |1715915             |58254392.2500    |8705875.3000    |\n",
      "|2022-10-30    |459105          |1904604             |64367132.7500    |9647234.4500    |\n",
      "|2022-10-29    |465857          |1932896             |65379979.0000    |9797068.6500    |\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check updated daily metrics using Snowpark\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Updated daily business metrics (most recent dates):\")\n",
    "session.table(\"tasty_bytes_db.analytics.daily_business_metrics\") \\\n",
    "    .select(\"order_date\", \"total_orders\", \"total_items_sold\", \"total_revenue\", \"total_profit\") \\\n",
    "    .order_by(F.col(\"order_date\").desc()) \\\n",
    "    .limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated product performance (top products by revenue):\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_NAME\"            |\"ITEM_CATEGORY\"  |\"TOTAL_UNITS_SOLD\"  |\"TOTAL_REVENUE\"  |\"TOTAL_PROFIT\"  |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "|The King Combo              |Main             |21563442            |431268886.0000   |172507552.0000  |\n",
      "|Tandoori Mixed Grill        |Main             |20659524            |371871477.0000   |144616729.2000  |\n",
      "|Lean Chicken Tikka Masala   |Main             |20660242            |351224142.9000   |144621673.6000  |\n",
      "|Spicy Miso Vegetable Ramen  |Main             |18897705            |325985430.2250   |193701398.6250  |\n",
      "|Tonkotsu Ramen              |Main             |18896002            |325956029.3250   |193683989.4500  |\n",
      "|Creamy Chicken Ramen        |Main             |18894956            |325938006.5250   |174778320.5750  |\n",
      "|Combination Curry           |Main             |20663383            |309950749.5000   |123980364.0000  |\n",
      "|Chicken Pot Pie Crepe       |Main             |20171577            |302573652.0000   |181544188.5000  |\n",
      "|Lobster Mac & Cheese        |Main             |17807346            |267110232.0000   |89036731.5000   |\n",
      "|Gyro Plate                  |Main             |21562685            |258752204.4000   |86250728.0000   |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check updated product performance using Snowpark\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Updated product performance (top products by revenue):\")\n",
    "session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .select(\"menu_item_name\", \"item_category\", \"total_units_sold\", \"total_revenue\", \"total_profit\") \\\n",
    "    .order_by(F.col(\"total_revenue\").desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Refresh Summary\n",
    "\n",
    "View the latest refresh operation for each dynamic table, including duration and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest refresh operations for all dynamic tables:\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"NAME\"                       |\"REFRESH_ACTION\"  |\"STATE\"    |\"REFRESH_START_TIME\"              |\"REFRESH_END_TIME\"                |\"REFRESH_DURATION_SECONDS\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|DAILY_BUSINESS_METRICS       |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:42.306000-08:00  |2025-12-24 13:34:43.662000-08:00  |1                           |\n",
      "|ORDERS_ENRICHED              |NO_DATA           |SUCCEEDED  |2025-12-24 13:34:44.275000-08:00  |2025-12-24 13:34:44.774000-08:00  |0                           |\n",
      "|ORDER_FACT                   |NO_DATA           |SUCCEEDED  |2025-12-24 13:34:45.008000-08:00  |2025-12-24 13:34:45.487000-08:00  |0                           |\n",
      "|ORDER_ITEMS_ENRICHED         |NO_DATA           |SUCCEEDED  |2025-12-24 13:34:44.278000-08:00  |2025-12-24 13:34:44.774000-08:00  |0                           |\n",
      "|PRODUCT_PERFORMANCE_METRICS  |INCREMENTAL       |SUCCEEDED  |2025-12-24 13:34:45.721000-08:00  |2025-12-24 13:35:09.055000-08:00  |24                          |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary of latest refresh operations\n",
    "print(\"Latest refresh operations for all dynamic tables:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT\n",
    "        name,\n",
    "        refresh_action,\n",
    "        state,\n",
    "        refresh_start_time,\n",
    "        refresh_end_time,\n",
    "        DATEDIFF('second', refresh_start_time, refresh_end_time) AS refresh_duration_seconds\n",
    "    FROM (\n",
    "        SELECT \n",
    "            name, \n",
    "            refresh_action, \n",
    "            state, \n",
    "            refresh_start_time, \n",
    "            refresh_end_time,\n",
    "            ROW_NUMBER() OVER (PARTITION BY name ORDER BY refresh_start_time DESC) as rn\n",
    "        FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY())\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "    ORDER BY name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: AI-Powered Insights ðŸ¤– *(Optional)*\n",
    "\n",
    "Create an AI agent using Snowflake Intelligence to answer natural language questions about your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—ï¸ Create Agent Infrastructure\n",
    "\n",
    "Set up the database and schema for Snowflake Intelligence agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileges granted for Snowflake Intelligence\n",
      "Database 'snowflake_intelligence' created\n",
      "Schema 'agents' created\n",
      "Agent infrastructure created\n"
     ]
    }
   ],
   "source": [
    "# Grant required privileges (requires ACCOUNTADMIN)\n",
    "session.use_role(\"ACCOUNTADMIN\")\n",
    "session.sql(\"GRANT CREATE DATABASE ON ACCOUNT TO ROLE pydata_lab_role\").collect()\n",
    "\n",
    "# Switch back to pydata_lab_role\n",
    "session.use_role(\"pydata_lab_role\")\n",
    "print(\"Privileges granted for Snowflake Intelligence\")\n",
    "\n",
    "# Create Intelligence database using Python API\n",
    "intelligence_db = Database(name=\"snowflake_intelligence\")\n",
    "root.databases.create(intelligence_db, mode=CreateMode.or_replace)\n",
    "print(\"Database 'snowflake_intelligence' created\")\n",
    "\n",
    "# Create agents schema using Python API\n",
    "intelligence_db_ref = root.databases[\"snowflake_intelligence\"]\n",
    "agents_schema = Schema(name=\"agents\")\n",
    "intelligence_db_ref.schemas.create(agents_schema, mode=CreateMode.or_replace)\n",
    "print(\"Schema 'agents' created\")\n",
    "\n",
    "# Grant permissions\n",
    "session.sql(\"GRANT USAGE ON DATABASE snowflake_intelligence TO ROLE pydata_lab_role\").collect()\n",
    "session.sql(\"GRANT USAGE ON SCHEMA snowflake_intelligence.agents TO ROLE pydata_lab_role\").collect()\n",
    "session.sql(\"GRANT CREATE AGENT ON SCHEMA snowflake_intelligence.agents TO ROLE pydata_lab_role\").collect()\n",
    "\n",
    "print(\"Agent infrastructure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ–¥ï¸ Create Agent via UI\n",
    "\n",
    "Follow the instructor's guidance to create an agent through the Snowflake UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¬ Sample Questions for AI Agent\n",
    "\n",
    "Once your agent is connected to the semantic model, try asking:\n",
    "\n",
    "| Category | Example Questions |\n",
    "|----------|-------------------|\n",
    "| **Revenue** | \"What was the total revenue for the last 30 days?\" |\n",
    "| **Products** | \"Which products have the highest profit margins?\" |\n",
    "| **Trends** | \"Show me daily revenue trends as a line chart\" |\n",
    "| **Customers** | \"How many unique customers did we have yesterday?\" |\n",
    "| **Discounts** | \"What percentage of orders include discounts?\" |\n",
    "| **Categories** | \"Compare revenue by product category\" |\n",
    "| **Performance** | \"Show me the top 5 products by profit\" |\n",
    "| **Timing** | \"What are the busiest hours for orders?\" |\n",
    "| **Patterns** | \"How does revenue vary by day of week?\" |\n",
    "| **Brands** | \"Which truck brands generate the most profit?\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Verify Agent Responses\n",
    "\n",
    "Use these queries to validate AI agent responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 products by revenue (using Snowpark):\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_NAME\"            |\"ITEM_CATEGORY\"  |\"TOTAL_REVENUE\"  |\"TOTAL_PROFIT\"  |\"AVG_PROFIT_MARGIN_PCT\"  |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "|The King Combo              |Main             |431268886.0000   |172507552.0000  |40.00                    |\n",
      "|Tandoori Mixed Grill        |Main             |371871477.0000   |144616729.2000  |38.89                    |\n",
      "|Lean Chicken Tikka Masala   |Main             |351224142.9000   |144621673.6000  |41.18                    |\n",
      "|Spicy Miso Vegetable Ramen  |Main             |325985430.2250   |193701398.6250  |59.42                    |\n",
      "|Tonkotsu Ramen              |Main             |325956029.3250   |193683989.4500  |59.42                    |\n",
      "|Creamy Chicken Ramen        |Main             |325938006.5250   |174778320.5750  |53.62                    |\n",
      "|Combination Curry           |Main             |309950749.5000   |123980364.0000  |40.00                    |\n",
      "|Chicken Pot Pie Crepe       |Main             |302573652.0000   |181544188.5000  |60.00                    |\n",
      "|Lobster Mac & Cheese        |Main             |267110232.0000   |89036731.5000   |33.33                    |\n",
      "|Gyro Plate                  |Main             |258752204.4000   |86250728.0000   |33.33                    |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 products by revenue using Snowpark\n",
    "print(\"Top 10 products by revenue (using Snowpark):\")\n",
    "top_products = session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .select(\n",
    "        F.col(\"menu_item_name\"),\n",
    "        F.col(\"item_category\"),\n",
    "        F.col(\"total_revenue\"),\n",
    "        F.col(\"total_profit\"),\n",
    "        F.col(\"avg_profit_margin_pct\")\n",
    "    ) \\\n",
    "    .order_by(F.col(\"total_revenue\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue by product category (using Snowpark):\n",
      "---------------------------------------------------------------------------\n",
      "|\"ITEM_CATEGORY\"  |\"CATEGORY_REVENUE\"  |\"CATEGORY_PROFIT\"  |\"UNITS_SOLD\"  |\n",
      "---------------------------------------------------------------------------\n",
      "|Main             |8684728559.6750     |4315773045.1000    |711451561     |\n",
      "|Snack            |660730871.6000      |390646235.0500     |79299243      |\n",
      "|Beverage         |506821161.1500      |396277469.7500     |189178359     |\n",
      "|Dessert          |258586766.9000      |168916568.5500     |50050934      |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate revenue by category using Snowpark aggregation\n",
    "print(\"Revenue by product category (using Snowpark):\")\n",
    "category_revenue = session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .group_by(\"item_category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "        F.sum(\"total_profit\").alias(\"category_profit\"),\n",
    "        F.sum(\"total_units_sold\").alias(\"units_sold\")\n",
    "    ) \\\n",
    "    .order_by(F.col(\"category_revenue\").desc())\n",
    "\n",
    "category_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily revenue trend (last 30 days):\n",
      "-----------------------------------------------------------------------------------------\n",
      "|\"ORDER_DATE\"  |\"TOTAL_ORDERS\"  |\"TOTAL_REVENUE\"  |\"TOTAL_PROFIT\"  |\"UNIQUE_CUSTOMERS\"  |\n",
      "-----------------------------------------------------------------------------------------\n",
      "|2025-12-24    |1200            |171300.4000      |25864.4000      |66                  |\n",
      "|2022-11-01    |425886          |59793523.2500    |8961352.6000    |18644               |\n",
      "|2022-10-31    |412664          |58254392.2500    |8705875.3000    |15854               |\n",
      "|2022-10-30    |459105          |64367132.7500    |9647234.4500    |18020               |\n",
      "|2022-10-29    |465857          |65379979.0000    |9797068.6500    |18145               |\n",
      "|2022-10-28    |448874          |63159728.0000    |9464734.6500    |21603               |\n",
      "|2022-10-27    |439475          |61790017.0000    |9256160.4500    |18029               |\n",
      "|2022-10-26    |432436          |60760310.2500    |9102695.2000    |16650               |\n",
      "|2022-10-25    |431288          |60639247.2500    |9080964.4000    |20235               |\n",
      "|2022-10-24    |407548          |57434597.2500    |8597204.9500    |18816               |\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get daily revenue trend for last 30 days using Snowpark\n",
    "print(\"Daily revenue trend (last 30 days):\")\n",
    "daily_trend = session.table(\"tasty_bytes_db.analytics.daily_business_metrics\") \\\n",
    "    .select(\n",
    "        F.col(\"order_date\"),\n",
    "        F.col(\"total_orders\"),\n",
    "        F.col(\"total_revenue\"),\n",
    "        F.col(\"total_profit\"),\n",
    "        F.col(\"unique_customers\")\n",
    "    ) \\\n",
    "    .order_by(F.col(\"order_date\").desc()) \\\n",
    "    .limit(30)\n",
    "\n",
    "daily_trend.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ§¹ Cleanup (Optional)\n",
    "\n",
    "> âš ï¸ **Warning:** This will permanently delete all data and objects created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to perform cleanup\n",
    "\n",
    "# # Drop databases\n",
    "# root.databases[\"tasty_bytes_db\"].delete()\n",
    "# print(\"Database 'tasty_bytes_db' dropped\")\n",
    "\n",
    "# root.databases[\"snowflake_intelligence\"].delete()\n",
    "# print(\"Database 'snowflake_intelligence' dropped\")\n",
    "\n",
    "# # Drop warehouse\n",
    "# root.warehouses[\"PYDATA_LAB_WH\"].delete()\n",
    "# print(\"Warehouse 'PYDATA_LAB_WH' dropped\")\n",
    "\n",
    "# # Optionally drop the role\n",
    "# # session.use_role(\"ACCOUNTADMIN\")\n",
    "# # session.sql(\"DROP ROLE IF EXISTS pydata_lab_role\").collect()\n",
    "# # print(\"Role 'pydata_lab_role' dropped\")\n",
    "\n",
    "print(\"Cleanup section ready (uncomment to execute)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the session\n",
    "session.close()\n",
    "print(\"Session closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸŽ‰ Summary & Resources\n",
    "\n",
    "## What We Built\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **End-to-End Pipeline** | Ingests raw data â†’ transforms automatically â†’ delivers fresh insights |\n",
    "| **No Orchestration Code** | Dynamic Tables handle scheduling, dependencies, and incremental processing |\n",
    "| **AI-Ready Data** | Semantic models enable natural language queries |\n",
    "\n",
    "## Technologies Used\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| **AWS S3** | Cloud storage for raw data files |\n",
    "| **Snowflake Python APIs** | Database/schema/table management |\n",
    "| **Snowpark DataFrames** | Data querying and transformation |\n",
    "| **Dynamic Tables** | Declarative pipeline orchestration |\n",
    "| **Snowflake Intelligence** | AI-powered insights |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- [Snowflake Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)\n",
    "- [Snowpark Python Developer Guide](https://docs.snowflake.com/en/developer-guide/snowpark/python/index)\n",
    "- [Snowflake Python API Reference](https://docs.snowflake.com/developer-guide/snowflake-python-api/reference/latest/index)\n",
    "- [Snowflake Intelligence](https://docs.snowflake.com/en/user-guide/snowflake-cortex/snowflake-intelligence)\n",
    "\n",
    "### Learning\n",
    "- [Coursera: Snowflake Data Engineering Professional Certificate](https://www.coursera.org/professional-certificates/snowflake-data-engineering)\n",
    "- [Snowflake Developers YouTube](https://www.youtube.com/channel/UCxgY7r-o_ql8ADIdyiQr3Zw)\n",
    "- [Snowflake Developer Hub](https://developers.snowflake.com)\n",
    "\n",
    "### Community\n",
    "- [Snowflake-Labs GitHub](https://github.com/Snowflake-Labs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineering_snowflake_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
