{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Ingestion into Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ SETUP ZONE ðŸš§\n",
    "\n",
    "### Environment Options\n",
    "\n",
    "**Option A:** If you are comfortable locally navigating the terminal, Jupyter notebooks, Python versions, using pip (or conda), and/or creating and managing virtual environments, follow the instructions in the cell marked \"Option A: Setup Instructions\" below. **This option is for intermediate to advanced Pythonistas.**\n",
    "\n",
    "**Option B:** If you'd prefer **NOT** to deal with locally navigating the terminal, Jupyter notebooks, Python versions, pip (or conda), and/or virtual environments, simply log into your Snowflake account and easily import this notebook into your Snowflake environment. You can do this by following the instructions in the cell marked \"Option B: Setup instructions\" below. **This option is for beginner to intermediate to otherwise can't-be-bothered Pythonistas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Setup Instructions\n",
    "\n",
    "1. [Download the notebook here](https://tinyurl.com/pydata2025-nb) (OR clone the repo to your computer and navigate to it in the terminal):\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/Snowflake-Labs/pydata_boston_2025_notebook_to_pipeline.git && cd pydata_boston_2025_notebook_to_pipeline\n",
    "```\n",
    "\n",
    "2. Create and activate a Python virtual environment to isolate dependencies:\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python3 -m venv pydataboston_2025\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On macOS/Linux:\n",
    "source pydataboston_2025/bin/activate\n",
    "\n",
    "# On Windows:\n",
    "pydataboston_2025\\Scripts\\activate\n",
    "```\n",
    "\n",
    "3. Install all required packages using pip:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Alternative:** Install packages individually:\n",
    "\n",
    "```bash\n",
    "pip install snowflake-snowpark-python>=1.11.0\n",
    "pip install snowflake-core>=0.6.0\n",
    "pip install pandas>=1.5.0\n",
    "```\n",
    "\n",
    "4. Verify that the Snowflake packages are installed correctly:\n",
    "\n",
    "```python\n",
    "python3 -c \"from snowflake.snowpark import Session; from snowflake.core import Root; print('Installation successful!')\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Setup Instructions\n",
    "\n",
    "1. Log into the Snowflake account you created.\n",
    "\n",
    "2. In the left menu, hover over **Projects** and click on **Notebooks**.\n",
    "\n",
    "3. At the top right, click on the dropdown on the **+ Notebook** button, and select \"import .ipynb file\".\n",
    "\n",
    "4. Fill out the modal:\n",
    "\n",
    "- Give the notebook a name\n",
    "- Select **SNOWFLAKE_LEARNING_DB** as your database, and **PUBLIC** as the schema\n",
    "- For runtime, select **Run on warehouse**\n",
    "- Leave everything else as-is\n",
    "\n",
    "5. Click **Create**. Standby for further instructions.\n",
    "\n",
    "6. Using the **Packages** drop down at the top, search for `snowflake` and click on it. This will install the `snowflake` Python package in the notebook environment. Next, search for `snowflake-snowpark-python` and click on it. This will install Snowpark in your notebook environment.\n",
    "\n",
    "7. Proceed to the cell 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… END OF SETUP ZONE âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started! I'll be using Option A, but I'll be able to guide those who are using either Option A or Option B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the necessary libraries. We'll use:\n",
    "\n",
    "- `snowflake.core` for data operations using Snowflake's Python APIs and Snowflake's compute engine (won't run into any local memory errors)\n",
    "\n",
    "- `snowflake.snowpark` for Python DataFrame operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session # For Snowflake compute engine connection\n",
    "from snowflake.core import Root, CreateMode # For core operations\n",
    "from snowflake.core.database import Database # For database operations\n",
    "from snowflake.core.schema import Schema # For schema operations\n",
    "from snowflake.core.warehouse import Warehouse # For compute engine operations\n",
    "from snowflake.core.table import Table, TableColumn, TableCollection # For table operations\n",
    "from snowflake.core.role import Role, Securable # For role operations\n",
    "from snowflake.core.stage import Stage, StageDirectoryTable # For stage operations\n",
    "from snowflake.core.dynamic_table import DynamicTable, DownstreamLag, UserDefinedLag, DynamicTableCollection # For dynamic table operations\n",
    "from snowflake.snowpark.types import * # For defining DataFrame schema\n",
    "from snowflake.snowpark import functions as F # For DataFrame functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Snowflake\n",
    "\n",
    "We'lls use Snowflake's compute engine for this lab. This way we can take advantage of distributed computing and not be bounded by any limits of our computer's local memory when loading or processing (transforming) data. \n",
    "\n",
    "Start by creating a session to connect to Snowflake:\n",
    "\n",
    "You can either:\n",
    "\n",
    "1. Pass credentials directly in a connection parameters dictionary (not recommended for production â€“ but we will do this just for today's lab) \n",
    "2. Use a connection configuration file using Snowflake CLI\n",
    "3. Use environment variables or external authentication\n",
    "\n",
    "For this tutorial, **we'll use a connection parameters dictionary**. Again, this is not recommended for production, but we will do this only for today's tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snowflake Login](./img/snowflake_login.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„ï¸ Connected to Snowflake account: \"DJKRPAC-QHB69982\"\n",
      "Current role: \"ACCOUNTADMIN\"\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL FOR OPTION A ONLY\n",
    "\n",
    "# Replace these with your Snowflake account details\n",
    "# Follow along with Gilberto for guidance on how to fill these out\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": \"DJKRPAC-QHB69982\",\n",
    "    \"user\": \"JOAOMARIO001\",\n",
    "    \"password\": \"c4auehbKxWAD7nq*R\",\n",
    "    \"role\": \"ACCOUNTADMIN\",\n",
    "    \"warehouse\": \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "# Create a session\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(f\"â„ï¸ Connected to Snowflake account: {session.get_current_account()}\")\n",
    "print(f\"Current role: {session.get_current_role()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CELL FOR OPTION B ONLY\n",
    "\n",
    "# from snowflake.snowpark.context import get_active_session\n",
    "# session = get_active_session()\n",
    "\n",
    "# print(f\"â„ï¸ Connected to Snowflake account: {session.get_current_account()}\")\n",
    "# print(f\"Current role: {session.get_current_role()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Root API Object\n",
    "\n",
    "The Root object is the entry point for Snowflake's Python API. It provides access to all database objects and operations. We must instantiate `Root` to be able to do anything with the data in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root API object created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create root object from Root():\n",
    "root = Root(session)\n",
    "print(\"Root API object created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dedicated Lab Role Using Python API\n",
    "\n",
    "We'll create a dedicated role for this lab with the necessary privileges using the Snowflake Python API. This follows the principle of least privilege, meaning that a user in a data platform should have only the minimum required privileges to execute specific jobs or tasks. **In practice, this is known as \"Role Based Access Control\", frequently referred to as \"RBAC\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role 'pydata_lab_role' created successfully\n",
      "Privileges granted to pydata_lab_role\n"
     ]
    }
   ],
   "source": [
    "# Create lab role using Python API\n",
    "pydata_lab_role = Role(name=\"pydata_lab_role\")\n",
    "root.roles.create(pydata_lab_role, mode=CreateMode.if_not_exists)\n",
    "print(\"Role 'pydata_lab_role' created successfully\")\n",
    "\n",
    "# Grant role to SYSADMIN (SQL required for role grants)\n",
    "session.sql(\"GRANT ROLE pydata_lab_role TO ROLE SYSADMIN\").collect()\n",
    "\n",
    "# Grant necessary privileges to role (SQL required for account-level grants)\n",
    "session.sql(\"GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE pydata_lab_role\").collect()\n",
    "session.sql(\"GRANT CREATE DATABASE ON ACCOUNT TO ROLE pydata_lab_role\").collect()\n",
    "\n",
    "print(\"Privileges granted to pydata_lab_role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch to/assume lab role\n",
    "\n",
    "Now we'll assume our new **pydata_lab_role** for all subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to role: \"PYDATA_LAB_ROLE\"\n"
     ]
    }
   ],
   "source": [
    "session.use_role(\"pydata_lab_role\")\n",
    "print(f\"Switched to role: {session.get_current_role()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review data pipeline architecture\n",
    "\n",
    "Let's quickly review the reference architecture diagram to understand how we'll build the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define data objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Database Using Python API\n",
    "\n",
    "We'll create a database called `tasty_bytes_db`. This database will contain all of the Tasty Bytes sales data that we'll use to build our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'tasty_bytes_db' created successfully\n",
      "Session is now using database: \"TASTY_BYTES_DB\"\n"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "database_name = \"tasty_bytes_db\"\n",
    "new_database = Database(name=database_name)\n",
    "root.databases.create(new_database, mode=CreateMode.or_replace)\n",
    "\n",
    "print(f\"Database '{database_name}' created successfully\")\n",
    "\n",
    "# Set database context\n",
    "session.use_database(database_name)\n",
    "print(f\"Session is now using database: {session.get_current_database()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schemas\n",
    "\n",
    "A schema is a grouping of data within a database. They're used to logically group related sets of data. For this lab, we'll create two schemas: \n",
    "\n",
    "* `raw` (for source data)\n",
    "* `analytics` (for transformed data)\n",
    "\n",
    "This separation of raw and transformed data is a common pattern in the field of data engineering. Schemas can be named anything you want.\n",
    "\n",
    "![Schemas Creation](./img/schemas_creation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema 'raw' created successfully\n",
      "Schema 'analytics' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Get database reference\n",
    "db = root.databases[database_name]\n",
    "\n",
    "# Create raw schema\n",
    "raw_schema = Schema(name=\"raw\")\n",
    "db.schemas.create(raw_schema, mode=CreateMode.or_replace)\n",
    "print(\"Schema 'raw' created successfully\")\n",
    "\n",
    "# Create analytics schema using Python API:\n",
    "analytics_schema = Schema(name=\"analytics\")\n",
    "db.schemas.create(analytics_schema, mode=CreateMode.or_replace)\n",
    "print(\"Schema 'analytics' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Resource for Processing Data\n",
    "\n",
    "A virtual warehouse provides compute resources for query execution in Snowflake. We'll create an X-Large warehouse with auto-suspend to optimize costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Virtual Warehouse](./img/virtual_wh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse 'pydata_lab_wh' created successfully\n",
      "Granted USAGE on warehouse to pydata_lab_role\n",
      "Using virtual warehouse: \"PYDATA_LAB_WH\"\n"
     ]
    }
   ],
   "source": [
    "# Create compute resource\n",
    "virtual_warehouse_name = \"pydata_lab_wh\"\n",
    "\n",
    "new_warehouse = Warehouse(\n",
    "    name=virtual_warehouse_name,\n",
    "    warehouse_size=\"XLARGE\"\n",
    ")\n",
    "\n",
    "root.warehouses.create(new_warehouse, mode=CreateMode.or_replace)\n",
    "print(f\"Warehouse '{virtual_warehouse_name}' created successfully\")\n",
    "\n",
    "# Grant usage on warehouse to pydata_lab_role\n",
    "root.roles[\"pydata_lab_role\"].grant_privileges(\n",
    "    privileges=[\"USAGE\"],\n",
    "    securable_type=\"WAREHOUSE\",\n",
    "    securable=Securable(name=virtual_warehouse_name)\n",
    ")\n",
    "print(f\"Granted USAGE on warehouse to pydata_lab_role\")\n",
    "\n",
    "# Use the warehouse\n",
    "session.use_warehouse(virtual_warehouse_name)\n",
    "print(f\"Using virtual warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create External Stages Using Python API\n",
    "\n",
    "Stages define locations where **data files** are stored. We'll create an external stage that points to a public S3 bucket. This bucket will contain the CSV data files containing the raw data we want to use to build our pipeline.\n",
    "\n",
    "Using cloud object storage (e.g., AWS S3 bucket) for storing raw data files is a very common practice in data engineering. A common pattern is to land raw data files in cloud object storage and use data platforms to ingest the data in the data files into the data platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stages](./img/stages.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External stage 'tasty_bytes_stage' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create external stage\n",
    "tasty_bytes_stage = Stage(\n",
    "    name=\"tasty_bytes_stage\",\n",
    "    url=\"s3://sfquickstarts/tasty-bytes-builder-education/\",\n",
    "    directory_table=StageDirectoryTable(enable=True)\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"raw\"].stages.create(\n",
    "    tasty_bytes_stage,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "print(\"External stage 'tasty_bytes_stage' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Raw Tables Using Python API\n",
    "\n",
    "Let's now define the tables that will hold our raw data. We'll define three rwa tables, and together, you can think of them as the \"raw zone\" within our data platform, containing the data we loaded from the source CSV files in AWS S3.\n",
    "\n",
    "The three \"raw zone\" tables:\n",
    "\n",
    "1. `order_header` - Order-level information\n",
    "\n",
    "2. `order_detail` - Line item information\n",
    "\n",
    "3. `menu` - Product catalog\n",
    "\n",
    "Across these three tables, there are nearly **1 billion rows of raw data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'order_header' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Get schema reference\n",
    "raw_schema_ref = db.schemas[\"raw\"]\n",
    "\n",
    "# Define order_header table structure\n",
    "order_header_columns = [\n",
    "    TableColumn(name=\"order_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"truck_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"location_id\", datatype=\"FLOAT\"),\n",
    "    TableColumn(name=\"customer_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"discount_id\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"shift_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"shift_start_time\", datatype=\"TIME(9)\"),\n",
    "    TableColumn(name=\"shift_end_time\", datatype=\"TIME(9)\"),\n",
    "    TableColumn(name=\"order_channel\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_ts\", datatype=\"TIMESTAMP_NTZ(9)\"),\n",
    "    TableColumn(name=\"served_ts\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_currency\", datatype=\"VARCHAR(3)\"),\n",
    "    TableColumn(name=\"order_amount\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"order_tax_amount\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_discount_amount\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"order_total\", datatype=\"NUMBER(38,4)\")\n",
    "]\n",
    "\n",
    "# Create order_header table using Python API\n",
    "order_header_table = Table(\n",
    "    name=\"order_header\",\n",
    "    columns=order_header_columns\n",
    ")\n",
    "\n",
    "raw_schema_ref.tables.create(order_header_table, mode=CreateMode.or_replace)\n",
    "print(\"Table 'order_header' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'order_detail' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Define order_detail table structure\n",
    "order_detail_columns = [\n",
    "    TableColumn(name=\"order_detail_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"order_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"menu_item_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"discount_id\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"line_number\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"quantity\", datatype=\"NUMBER(5,0)\"),\n",
    "    TableColumn(name=\"unit_price\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"price\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"order_item_discount_amount\", datatype=\"VARCHAR(16777216)\")\n",
    "]\n",
    "\n",
    "# Create order_detail table\n",
    "order_detail_table = Table(\n",
    "    name=\"order_detail\",\n",
    "    columns=order_detail_columns\n",
    ")\n",
    "\n",
    "raw_schema_ref.tables.create(order_detail_table, mode=CreateMode.or_replace)\n",
    "print(\"Table 'order_detail' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'menu' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Define menu table structure\n",
    "menu_columns = [\n",
    "    TableColumn(name=\"menu_id\", datatype=\"NUMBER(19,0)\"),\n",
    "    TableColumn(name=\"menu_type_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"menu_type\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"truck_brand_name\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"menu_item_id\", datatype=\"NUMBER(38,0)\"),\n",
    "    TableColumn(name=\"menu_item_name\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"item_category\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"item_subcategory\", datatype=\"VARCHAR(16777216)\"),\n",
    "    TableColumn(name=\"cost_of_goods_usd\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"sale_price_usd\", datatype=\"NUMBER(38,4)\"),\n",
    "    TableColumn(name=\"menu_item_health_metrics_obj\", datatype=\"VARIANT\")\n",
    "]\n",
    "\n",
    "# Create menu table\n",
    "menu_table = Table(\n",
    "    name=\"menu\",\n",
    "    columns=menu_columns\n",
    ")\n",
    "\n",
    "raw_schema_ref.tables.create(menu_table, mode=CreateMode.or_replace)\n",
    "print(\"Table 'menu' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2: Ingest data into data objects**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate the raw zone tables with data using COPY_INTO_TABLE\n",
    "\n",
    "Now we'll load data from the files in external stage into our raw tables using Snowpark's `copy_into_table` method.\n",
    "\n",
    "The dataset contains approximately 1 billion records across all tables. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading order_header data...\n",
      "Loaded 179 batch(es) into order_header\n"
     ]
    }
   ],
   "source": [
    "# Load order_header data using Snowpark copy_into_table\n",
    "print(\"Loading order_header data...\")\n",
    "\n",
    "order_header_df = session.read.csv(\"@tasty_bytes_db.raw.tasty_bytes_stage/raw_pos/order_header/\")\n",
    "\n",
    "# Map $1, $2, $3... to the actual columns by position\n",
    "result = order_header_df.copy_into_table(\n",
    "    \"tasty_bytes_db.raw.order_header\",  \n",
    "    target_columns=[\"order_id\", \"truck_id\", \"location_id\", \"customer_id\", \"discount_id\", \n",
    "                    \"shift_id\", \"shift_start_time\", \"shift_end_time\", \"order_channel\", \n",
    "                    \"order_ts\", \"served_ts\", \"order_currency\", \"order_amount\", \n",
    "                    \"order_tax_amount\", \"order_discount_amount\", \"order_total\"],\n",
    "    transformations=[\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \n",
    "                     \"$9\", \"$10\", \"$11\", \"$12\", \"$13\", \"$14\", \"$15\", \"$16\"]\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(result)} batch(es) into order_header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading order_detail data...\n",
      "Successfully loaded 270 batch(es) into order_detail\n"
     ]
    }
   ],
   "source": [
    "# Load order_detail data using Snowpark copy_into_table\n",
    "print(\"Loading order_detail data...\")\n",
    "\n",
    "order_detail_df = session.read.csv(\"@tasty_bytes_db.raw.tasty_bytes_stage/raw_pos/order_detail/\")\n",
    "\n",
    "result = order_detail_df.copy_into_table(\n",
    "    \"tasty_bytes_db.raw.order_detail\",\n",
    "    target_columns=[\"order_detail_id\", \"order_id\", \"menu_item_id\", \"discount_id\", \n",
    "                    \"line_number\", \"quantity\", \"unit_price\", \"price\", \"order_item_discount_amount\"],\n",
    "    transformations=[\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \"$9\"]\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(result)} batch(es) into order_detail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading menu data...\n",
      "Successfully loaded 1 batch(es) into menu\n"
     ]
    }
   ],
   "source": [
    "# Load menu data using Snowpark copy_into_table\n",
    "print(\"Loading menu data...\")\n",
    "\n",
    "menu_df = session.read.csv(\"@tasty_bytes_db.raw.tasty_bytes_stage/raw_pos/menu/\")\n",
    "\n",
    "result = menu_df.copy_into_table(\n",
    "    \"tasty_bytes_db.raw.menu\",\n",
    "    target_columns=[\"menu_id\", \"menu_type_id\", \"menu_type\", \"truck_brand_name\", \"menu_item_id\",\n",
    "                    \"menu_item_name\", \"item_category\", \"item_subcategory\", \"cost_of_goods_usd\",\n",
    "                    \"sale_price_usd\", \"menu_item_health_metrics_obj\"],\n",
    "    transformations=[\"$1\", \"$2\", \"$3\", \"$4\", \"$5\", \"$6\", \"$7\", \"$8\", \"$9\", \"$10\", \"$11\"]\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(result)} batch(es) into menu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Data Loading Using Snowpark\n",
    "\n",
    "We've already verified the data was loaded correctly by checking row counts in the Snowflake UI. But we can do this programmatically as well using Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_header table: 248,201,269 rows\n",
      "order_detail table: 673,655,465 rows\n",
      "menu table: 100 rows\n"
     ]
    }
   ],
   "source": [
    "# Check row counts using Snowpark\n",
    "order_header_count = session.table(\"tasty_bytes_db.raw.order_header\").count()\n",
    "order_detail_count = session.table(\"tasty_bytes_db.raw.order_detail\").count()\n",
    "menu_count = session.table(\"tasty_bytes_db.raw.menu\").count()\n",
    "\n",
    "print(f\"order_header table: {order_header_count:,} rows\")\n",
    "print(f\"order_detail table: {order_detail_count:,} rows\")\n",
    "print(f\"menu table: {menu_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from order_header:\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"TRUCK_ID\"  |\"LOCATION_ID\"  |\"CUSTOMER_ID\"  |\"DISCOUNT_ID\"  |\"SHIFT_ID\"  |\"SHIFT_START_TIME\"  |\"SHIFT_END_TIME\"  |\"ORDER_CHANNEL\"  |\"ORDER_TS\"           |\"SERVED_TS\"  |\"ORDER_CURRENCY\"  |\"ORDER_AMOUNT\"  |\"ORDER_TAX_AMOUNT\"  |\"ORDER_DISCOUNT_AMOUNT\"  |\"ORDER_TOTAL\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|60938729    |191         |11541.0        |NULL           |NULL           |189043      |15:00:00            |22:00:00          |NULL             |2022-04-10 19:25:02  |NULL         |USD               |80.0000         |NULL                |NULL                     |80.0000        |\n",
      "|60938730    |191         |11541.0        |NULL           |NULL           |189043      |15:00:00            |22:00:00          |NULL             |2022-04-10 19:25:34  |NULL         |USD               |82.0000         |NULL                |NULL                     |82.0000        |\n",
      "|60938731    |191         |11541.0        |NULL           |NULL           |189043      |15:00:00            |22:00:00          |NULL             |2022-04-10 19:26:11  |NULL         |USD               |41.0000         |NULL                |NULL                     |41.0000        |\n",
      "|60938732    |191         |11541.0        |NULL           |NULL           |189043      |15:00:00            |22:00:00          |NULL             |2022-04-10 19:27:18  |NULL         |USD               |12.0000         |NULL                |NULL                     |12.0000        |\n",
      "|60938733    |191         |11541.0        |NULL           |NULL           |189043      |15:00:00            |22:00:00          |NULL             |2022-04-10 19:27:19  |NULL         |USD               |55.0000         |NULL                |NULL                     |55.0000        |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View sample data from order_header using Snowpark (you can do this with the other table names too)\n",
    "print(\"Sample data from order_header:\")\n",
    "session.table(\"tasty_bytes_db.raw.order_header\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review data pipeline architecture\n",
    "\n",
    "Let's quickly review the reference architecture diagram to understand what we've built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Transform raw data into insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Dynamic Tables?\n",
    "\n",
    "Dynamic Tables are Snowflake's declarative approach to data transformation. Instead of writing imperative code (mutiple step-by-step scripts, tasks, streams, stored procedures) to manage data pipelines, you define the desired end state once and Snowflake handles:\n",
    "\n",
    "- Scheduling and orchestration: how often to refresh your transformed data\n",
    "- Incremental refresh logic: process only changed rows in source tables during refreshes\n",
    "- Automatic dependency tracking: native DAG in the Snowflake UI (we will see this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tier 1 Dynamic Table - Orders Enriched\n",
    "\n",
    "Now we begin building our transformations in our pipeline. The first tier dynamic tables enriches raw data with derived attributes.\n",
    "\n",
    "The `orders_enriched` dynamic table:\n",
    "- Extracts temporal dimensions (date, day of week, hour)\n",
    "- Converts discount amounts to numbers\n",
    "- Adds a discount flag\n",
    "\n",
    "We set `target_lag` to 12 hours (43200 seconds) as an example. This setting means Snowflake will refresh this table to be at most 12 hours behind the source data (the tables in our raw zone)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the dynamic table in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 1 dynamic table 'orders_enriched' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build orders_enriched Dynamic Table query using Snowpark DataFrame API\n",
    "orders_enriched_df = session.table(\"tasty_bytes_db.raw.order_header\") \\\n",
    "    .select(\n",
    "        # Order identifiers\n",
    "        F.col(\"order_id\"),\n",
    "        F.col(\"truck_id\"),\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"order_channel\"),\n",
    "        # Temporal dimensions\n",
    "        F.col(\"order_ts\").alias(\"order_timestamp\"),\n",
    "        F.to_date(F.col(\"order_ts\")).alias(\"order_date\"),\n",
    "        F.call_builtin(\"DAYNAME\", F.col(\"order_ts\")).alias(\"day_name\"),\n",
    "        F.hour(F.col(\"order_ts\")).alias(\"order_hour\"),\n",
    "        # Financial metrics\n",
    "        F.col(\"order_amount\"),\n",
    "        F.col(\"order_total\"),\n",
    "        F.call_builtin(\"TRY_TO_NUMBER\", F.col(\"order_discount_amount\"), F.lit(10), F.lit(2)).alias(\"order_discount_amount\"),\n",
    "        # Simple discount flag\n",
    "        F.when(\n",
    "            (F.col(\"discount_id\").isNotNull()) & (F.col(\"discount_id\") != \"\"),\n",
    "            F.lit(True)\n",
    "        ).otherwise(F.lit(False)).alias(\"has_discount\")\n",
    "    ) \\\n",
    "    .where(\n",
    "        F.col(\"order_id\").isNotNull() & F.col(\"order_ts\").isNotNull()\n",
    "    )\n",
    "\n",
    "# Create the dynamic table\n",
    "orders_enriched_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.orders_enriched\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"12 hours\"\n",
    ")\n",
    "\n",
    "print(\"Tier 1 dynamic table 'orders_enriched' created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...or build the dynamic table in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create orders_enriched dynamic table query\n",
    "orders_enriched_query = \"\"\"\n",
    "SELECT\n",
    "    -- Order identifiers\n",
    "    order_id,\n",
    "    truck_id,\n",
    "    customer_id,\n",
    "    order_channel,\n",
    "    -- Temporal dimensions\n",
    "    order_ts AS order_timestamp,\n",
    "    DATE(order_ts) AS order_date,\n",
    "    DAYNAME(order_ts) AS day_name,\n",
    "    HOUR(order_ts) AS order_hour,\n",
    "    -- Financial metrics\n",
    "    order_amount,\n",
    "    order_total,\n",
    "    TRY_TO_NUMBER(order_discount_amount, 10, 2) AS order_discount_amount,\n",
    "    -- Simple discount flag\n",
    "    CASE\n",
    "        WHEN discount_id IS NOT NULL AND discount_id != '' THEN TRUE\n",
    "        ELSE FALSE\n",
    "    END AS has_discount\n",
    "FROM tasty_bytes_db.raw.order_header\n",
    "WHERE order_id IS NOT NULL\n",
    "    AND order_ts IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "orders_enriched_dt = DynamicTable(\n",
    "    name=\"orders_enriched_sql\",\n",
    "    target_lag=UserDefinedLag(seconds=43200),  # 12 hours\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    query=orders_enriched_query\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "    orders_enriched_dt,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "\n",
    "print(\"Tier 1 dynamic table 'orders_enriched_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tier 1 Dynamic Table - Order Items Enriched\n",
    "\n",
    "The `order_items_enriched` dynamic table:\n",
    "\n",
    "- Joins order details with the menu to get product information\n",
    "- Calculates profit metrics (unit profit, line profit, profit margin)\n",
    "- Adds discount information\n",
    "\n",
    "This table also uses a 12-hour target lag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the dynamic table in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 1 dynamic table 'order_items_enriched' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build order_items_enriched using Snowpark DataFrame API\n",
    "od = session.table(\"tasty_bytes_db.raw.order_detail\")\n",
    "m = session.table(\"tasty_bytes_db.raw.menu\")\n",
    "\n",
    "order_items_enriched_df = od.join(m, od[\"menu_item_id\"] == m[\"menu_item_id\"], \"inner\") \\\n",
    "    .select(\n",
    "        # Order detail identifiers\n",
    "        od[\"order_detail_id\"],\n",
    "        od[\"order_id\"],\n",
    "        od[\"line_number\"],\n",
    "        # Product information\n",
    "        od[\"menu_item_id\"].alias(\"menu_item_id\"),\n",
    "        m[\"menu_item_name\"],\n",
    "        m[\"item_category\"],\n",
    "        m[\"item_subcategory\"],\n",
    "        m[\"truck_brand_name\"],\n",
    "        m[\"menu_type\"],\n",
    "        # Quantity and pricing\n",
    "        od[\"quantity\"],\n",
    "        od[\"unit_price\"],\n",
    "        od[\"price\"].alias(\"line_total\"),\n",
    "        m[\"cost_of_goods_usd\"],\n",
    "        m[\"sale_price_usd\"],\n",
    "        # Profit calculations\n",
    "        (od[\"unit_price\"] - m[\"cost_of_goods_usd\"]).alias(\"unit_profit\"),\n",
    "        ((od[\"unit_price\"] - m[\"cost_of_goods_usd\"]) * od[\"quantity\"]).alias(\"line_profit\"),\n",
    "        F.when(\n",
    "            od[\"unit_price\"] > 0,\n",
    "            F.round(((od[\"unit_price\"] - m[\"cost_of_goods_usd\"]) / od[\"unit_price\"]) * 100, 2)\n",
    "        ).otherwise(0).alias(\"profit_margin_pct\"),\n",
    "        # Discount information\n",
    "        F.call_builtin(\"TRY_TO_NUMBER\", od[\"order_item_discount_amount\"], F.lit(10), F.lit(2)).alias(\"line_discount_amount\"),\n",
    "        F.when(\n",
    "            (od[\"discount_id\"].isNotNull()) & (od[\"discount_id\"] != \"\"),\n",
    "            F.lit(True)\n",
    "        ).otherwise(F.lit(False)).alias(\"has_discount\")\n",
    "    ) \\\n",
    "    .where(\n",
    "        od[\"order_id\"].isNotNull() & od[\"menu_item_id\"].isNotNull()\n",
    "    )\n",
    "\n",
    "# Create the dynamic table\n",
    "order_items_enriched_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.order_items_enriched\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"12 hours\"\n",
    ")\n",
    "\n",
    "print(\"Tier 1 dynamic table 'order_items_enriched' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...or build the dynamic table in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create order_items_enriched dynamic table\n",
    "order_items_enriched_query = \"\"\"\n",
    "SELECT\n",
    "    -- Order detail identifiers\n",
    "    od.order_detail_id,\n",
    "    od.order_id,\n",
    "    od.line_number,\n",
    "    -- Product information\n",
    "    od.menu_item_id,\n",
    "    m.menu_item_name,\n",
    "    m.item_category,\n",
    "    m.item_subcategory,\n",
    "    m.truck_brand_name,\n",
    "    m.menu_type,\n",
    "    -- Quantity and pricing\n",
    "    od.quantity,\n",
    "    od.unit_price,\n",
    "    od.price AS line_total,\n",
    "    m.cost_of_goods_usd,\n",
    "    m.sale_price_usd,\n",
    "    -- Profit calculations\n",
    "    (od.unit_price - m.cost_of_goods_usd) AS unit_profit,\n",
    "    (od.unit_price - m.cost_of_goods_usd) * od.quantity AS line_profit,\n",
    "    CASE\n",
    "        WHEN od.unit_price > 0 THEN\n",
    "            ROUND(((od.unit_price - m.cost_of_goods_usd) / od.unit_price) * 100, 2)\n",
    "        ELSE 0\n",
    "    END AS profit_margin_pct,\n",
    "    -- Discount information\n",
    "    TRY_TO_NUMBER(od.order_item_discount_amount, 10, 2) AS line_discount_amount,\n",
    "    CASE\n",
    "        WHEN od.discount_id IS NOT NULL AND od.discount_id != '' THEN TRUE\n",
    "        ELSE FALSE\n",
    "    END AS has_discount\n",
    "FROM tasty_bytes_db.raw.order_detail od\n",
    "INNER JOIN tasty_bytes_db.raw.menu m\n",
    "    ON od.menu_item_id = m.menu_item_id\n",
    "WHERE od.order_id IS NOT NULL\n",
    "    AND od.menu_item_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "order_items_enriched_dt = DynamicTable(\n",
    "    name=\"order_items_enriched_sql\",\n",
    "    target_lag=UserDefinedLag(seconds=43200),  # 12 hours\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    query=order_items_enriched_query\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "    order_items_enriched_dt,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "\n",
    "print(\"Tier 1 dynamic table 'order_items_enriched_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Tier 1 Dynamic Tables Using Snowpark\n",
    "\n",
    "Let's verify that our Tier 1 dynamic tables were created and populated correctly using Snowpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from orders_enriched:\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"ORDER_DATE\"  |\"DAY_NAME\"  |\"ORDER_HOUR\"  |\"ORDER_AMOUNT\"  |\"ORDER_TOTAL\"  |\"HAS_DISCOUNT\"  |\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "|398127561   |2020-09-17    |Thu         |13            |13.0000         |13.0000        |False           |\n",
      "|398127688   |2020-09-17    |Thu         |17            |22.0000         |22.0000        |False           |\n",
      "|398127815   |2020-09-17    |Thu         |18            |13.0000         |13.0000        |False           |\n",
      "|398127942   |2020-09-17    |Thu         |19            |26.0000         |26.0000        |False           |\n",
      "|398128069   |2020-09-17    |Thu         |20            |13.0000         |13.0000        |False           |\n",
      "|398128196   |2020-09-18    |Fri         |8             |87.0000         |87.0000        |False           |\n",
      "|398128323   |2020-09-18    |Fri         |9             |38.0000         |38.0000        |False           |\n",
      "|398128450   |2020-09-18    |Fri         |11            |17.0000         |17.0000        |False           |\n",
      "|398128577   |2020-09-18    |Fri         |12            |17.0000         |17.0000        |False           |\n",
      "|398128704   |2020-09-18    |Fri         |13            |38.0000         |38.0000        |False           |\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check orders_enriched using Snowpark\n",
    "print(\"Sample data from orders_enriched:\")\n",
    "\n",
    "# Replace with orders_enriched_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.orders_enriched\") \\\n",
    "    .select(\"order_id\", \"order_date\", \"day_name\", \"order_hour\", \"order_amount\", \"order_total\", \"has_discount\") \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from order_items_enriched:\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_NAME\"         |\"ITEM_CATEGORY\"  |\"QUANTITY\"  |\"UNIT_PRICE\"  |\"LINE_TOTAL\"  |\"LINE_PROFIT\"  |\"PROFIT_MARGIN_PCT\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "|The Salad of All Salads  |Main             |1           |12.0000       |12.0000       |6.0000         |50.00                |\n",
      "|Bottled Soda             |Beverage         |1           |3.0000        |3.0000        |2.5000         |83.33                |\n",
      "|Seitan Buffalo Wings     |Snack            |1           |7.0000        |7.0000        |3.0000         |42.86                |\n",
      "|The Salad of All Salads  |Main             |1           |12.0000       |12.0000       |6.0000         |50.00                |\n",
      "|Veggie Burger            |Main             |1           |9.0000        |9.0000        |4.0000         |44.44                |\n",
      "|The Salad of All Salads  |Main             |1           |12.0000       |12.0000       |6.0000         |50.00                |\n",
      "|Veggie Burger            |Main             |1           |9.0000        |9.0000        |4.0000         |44.44                |\n",
      "|Bottled Soda             |Beverage         |2           |3.0000        |6.0000        |5.0000         |83.33                |\n",
      "|Seitan Buffalo Wings     |Snack            |4           |7.0000        |28.0000       |12.0000        |42.86                |\n",
      "|Seitan Buffalo Wings     |Snack            |1           |7.0000        |7.0000        |3.0000         |42.86                |\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check order_items_enriched using Snowpark\n",
    "print(\"Sample data from order_items_enriched:\")\n",
    "\n",
    "# Replace with orders_items_enriched_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.order_items_enriched\") \\\n",
    "    .select(\"menu_item_name\", \"item_category\", \"quantity\", \"unit_price\", \"line_total\", \"line_profit\", \"profit_margin_pct\") \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tier 2 Dynamic Table - Order Fact\n",
    "\n",
    "Tier 2 joins the enriched order headers with enriched order items to create a comprehensive fact table.\n",
    "\n",
    "Note the `target_lag=DownstreamLag()` setting. This tells Snowflake to refresh this table whenever its upstream dependencies (Tier 1 tables) are refreshed. This creates an automatic dependency graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the dynamic table in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier 2 Dynamic table 'order_fact' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build order_fact query using Snowpark DataFrame API\n",
    "o = session.table(\"tasty_bytes_db.analytics.orders_enriched\")\n",
    "oi = session.table(\"tasty_bytes_db.analytics.order_items_enriched\")\n",
    "\n",
    "order_fact_df = o.join(oi, o[\"order_id\"] == oi[\"order_id\"], \"inner\") \\\n",
    "    .select(\n",
    "        # Order header fields\n",
    "        o[\"order_id\"].alias(\"order_id\"),\n",
    "        o[\"truck_id\"],\n",
    "        o[\"customer_id\"],\n",
    "        o[\"order_channel\"],\n",
    "        o[\"order_timestamp\"],\n",
    "        o[\"order_date\"],\n",
    "        o[\"day_name\"],\n",
    "        o[\"order_hour\"],\n",
    "        o[\"order_amount\"],\n",
    "        o[\"order_total\"],\n",
    "        o[\"order_discount_amount\"].alias(\"order_level_discount\"),\n",
    "        o[\"has_discount\"].alias(\"order_has_discount\"),\n",
    "        # Order line item fields\n",
    "        oi[\"order_detail_id\"],\n",
    "        oi[\"line_number\"],\n",
    "        oi[\"menu_item_id\"],\n",
    "        oi[\"menu_item_name\"],\n",
    "        oi[\"item_category\"],\n",
    "        oi[\"item_subcategory\"],\n",
    "        oi[\"truck_brand_name\"],\n",
    "        oi[\"menu_type\"],\n",
    "        oi[\"quantity\"],\n",
    "        oi[\"unit_price\"],\n",
    "        oi[\"line_total\"],\n",
    "        oi[\"cost_of_goods_usd\"],\n",
    "        oi[\"sale_price_usd\"],\n",
    "        oi[\"unit_profit\"],\n",
    "        oi[\"line_profit\"],\n",
    "        oi[\"profit_margin_pct\"],\n",
    "        oi[\"line_discount_amount\"],\n",
    "        oi[\"has_discount\"].alias(\"line_has_discount\")\n",
    "    )\n",
    "\n",
    "# Create the dynamic table\n",
    "order_fact_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.order_fact\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"DOWNSTREAM\"\n",
    ")\n",
    "\n",
    "print(\"Tier 2 Dynamic table 'order_fact' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...or build the dynamic table in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create order_fact dynamic table query\n",
    "order_fact_query = \"\"\"\n",
    "SELECT\n",
    "    -- Order header fields\n",
    "    o.order_id,\n",
    "    o.truck_id,\n",
    "    o.customer_id,\n",
    "    o.order_channel,\n",
    "    o.order_timestamp,\n",
    "    o.order_date,\n",
    "    o.day_name,\n",
    "    o.order_hour,\n",
    "    o.order_amount,\n",
    "    o.order_total,\n",
    "    o.order_discount_amount AS order_level_discount,\n",
    "    o.has_discount AS order_has_discount,\n",
    "    -- Order line item fields\n",
    "    oi.order_detail_id,\n",
    "    oi.line_number,\n",
    "    oi.menu_item_id,\n",
    "    oi.menu_item_name,\n",
    "    oi.item_category,\n",
    "    oi.item_subcategory,\n",
    "    oi.truck_brand_name,\n",
    "    oi.menu_type,\n",
    "    oi.quantity,\n",
    "    oi.unit_price,\n",
    "    oi.line_total,\n",
    "    oi.cost_of_goods_usd,\n",
    "    oi.sale_price_usd,\n",
    "    oi.unit_profit,\n",
    "    oi.line_profit,\n",
    "    oi.profit_margin_pct,\n",
    "    oi.line_discount_amount,\n",
    "    oi.has_discount AS line_has_discount\n",
    "FROM tasty_bytes_db.analytics.orders_enriched_sql o\n",
    "INNER JOIN tasty_bytes_db.analytics.order_items_enriched_sql oi\n",
    "    ON o.order_id = oi.order_id\n",
    "\"\"\"\n",
    "\n",
    "order_fact_dt = DynamicTable(\n",
    "    name=\"order_fact_sql\",\n",
    "    target_lag=DownstreamLag(),  # Refresh when upstream tables refresh\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    query=order_fact_query\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "    order_fact_dt,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "\n",
    "print(\"Tier 2 Dynamic table 'order_fact_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Tier 2 Table Using Snowpark\n",
    "\n",
    "Let's check the order_fact table to ensure it contains the joined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from order_fact:\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_ID\"  |\"ORDER_DATE\"  |\"MENU_ITEM_NAME\"  |\"ITEM_CATEGORY\"  |\"QUANTITY\"  |\"ORDER_TOTAL\"  |\"LINE_PROFIT\"  |\"PROFIT_MARGIN_PCT\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "|63879385    |2022-10-26    |Combo Fried Rice  |Main             |3           |60.0000        |18.0000        |54.55                |\n",
      "|63879385    |2022-10-26    |Wonton Soup       |Main             |4           |60.0000        |16.0000        |66.67                |\n",
      "|63879385    |2022-10-26    |Bottled Soda      |Beverage         |1           |60.0000        |2.5000         |83.33                |\n",
      "|63879622    |2022-10-26    |Combo Lo Mein     |Main             |1           |13.0000        |7.0000         |53.85                |\n",
      "|63879696    |2022-10-26    |Combo Fried Rice  |Main             |3           |51.0000        |18.0000        |54.55                |\n",
      "|63879696    |2022-10-26    |Wonton Soup       |Main             |3           |51.0000        |12.0000        |66.67                |\n",
      "|63879863    |2022-10-26    |Wonton Soup       |Main             |2           |52.0000        |8.0000         |66.67                |\n",
      "|63879863    |2022-10-26    |Combo Fried Rice  |Main             |1           |52.0000        |6.0000         |54.55                |\n",
      "|63879863    |2022-10-26    |Ice Tea           |Beverage         |1           |52.0000        |2.2500         |75.00                |\n",
      "|63879863    |2022-10-26    |Combo Lo Mein     |Main             |2           |52.0000        |14.0000        |53.85                |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check order_fact using Snowpark\n",
    "print(\"Sample data from order_fact:\")\n",
    "\n",
    "# Replace with order_fact_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.order_fact\") \\\n",
    "    .select(\"order_id\", \"order_date\", \"menu_item_name\", \"item_category\", \"quantity\", \"order_total\", \"line_profit\", \"profit_margin_pct\") \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tier 3 Dynamic Table - Daily Business Metrics\n",
    "\n",
    "Tier 3 contains aggregated metrics. The first is `daily_business_metrics`, which pre-aggregates key business KPIs by day.\n",
    "\n",
    "This table aggregates:\n",
    "- Volume metrics (orders, trucks, customers, items sold)\n",
    "- Revenue metrics (total revenue, average order value)\n",
    "- Profit metrics (total profit, average margin)\n",
    "- Discount metrics (orders with discounts, discount amounts)\n",
    "\n",
    "It also uses `TARGET_LAG = 'DOWNSTREAM'` to refresh when order_fact changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tier 3) Dynamic table 'daily_business_metrics' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build daily_business_metrics using Snowpark DataFrame API\n",
    "daily_business_metrics_df = session.table(\"tasty_bytes_db.analytics.order_fact\") \\\n",
    "    .group_by(\"order_date\", \"day_name\") \\\n",
    "    .agg(\n",
    "        # Volume metrics\n",
    "        F.count_distinct(F.col(\"order_id\")).alias(\"total_orders\"),\n",
    "        F.count_distinct(F.col(\"truck_id\")).alias(\"active_trucks\"),\n",
    "        F.count_distinct(F.col(\"customer_id\")).alias(\"unique_customers\"),\n",
    "        F.sum(F.col(\"quantity\")).alias(\"total_items_sold\"),\n",
    "        # Revenue metrics\n",
    "        F.sum(F.col(\"order_total\")).alias(\"total_revenue\"),\n",
    "        F.round(F.avg(F.col(\"order_total\")), 2).alias(\"avg_order_value\"),\n",
    "        F.sum(F.col(\"line_total\")).alias(\"total_line_item_revenue\"),\n",
    "        # Profit metrics\n",
    "        F.sum(F.col(\"line_profit\")).alias(\"total_profit\"),\n",
    "        F.round(F.avg(F.col(\"profit_margin_pct\")), 2).alias(\"avg_profit_margin_pct\"),\n",
    "        # Discount metrics\n",
    "        F.sum(F.when(F.col(\"order_has_discount\"), 1).otherwise(0)).alias(\"orders_with_discount\"),\n",
    "        F.sum(F.col(\"order_level_discount\")).alias(\"total_order_discount_amount\"),\n",
    "        F.sum(F.col(\"line_discount_amount\")).alias(\"total_line_discount_amount\")\n",
    "    )\n",
    "\n",
    "# Create the dynamic table using Snowpark DataFrame method\n",
    "daily_business_metrics_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.daily_business_metrics\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"DOWNSTREAM\"\n",
    ")\n",
    "\n",
    "print(\"(Tier 3) Dynamic table 'daily_business_metrics' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...or build the dynamic table in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily_business_metrics dynamic table\n",
    "daily_business_metrics_query = \"\"\"\n",
    "SELECT\n",
    "    order_date,\n",
    "    day_name,\n",
    "    -- Volume metrics\n",
    "    COUNT(DISTINCT order_id) AS total_orders,\n",
    "    COUNT(DISTINCT truck_id) AS active_trucks,\n",
    "    COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "    SUM(quantity) AS total_items_sold,\n",
    "    -- Revenue metrics\n",
    "    SUM(order_total) AS total_revenue,\n",
    "    ROUND(AVG(order_total), 2) AS avg_order_value,\n",
    "    SUM(line_total) AS total_line_item_revenue,\n",
    "    -- Profit metrics\n",
    "    SUM(line_profit) AS total_profit,\n",
    "    ROUND(AVG(profit_margin_pct), 2) AS avg_profit_margin_pct,\n",
    "    -- Discount metrics\n",
    "    SUM(CASE WHEN order_has_discount THEN 1 ELSE 0 END) AS orders_with_discount,\n",
    "    SUM(order_level_discount) AS total_order_discount_amount,\n",
    "    SUM(line_discount_amount) AS total_line_discount_amount\n",
    "FROM tasty_bytes_db.analytics.order_fact_sql\n",
    "GROUP BY order_date, day_name\n",
    "\"\"\"\n",
    "\n",
    "daily_business_metrics_dt = DynamicTable(\n",
    "    name=\"daily_business_metrics_sql\",\n",
    "    target_lag=DownstreamLag(),  # Refresh when upstream tables refresh\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    query=daily_business_metrics_query\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "    daily_business_metrics_dt,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "\n",
    "print(\"(Tier 3) Dynamic table 'daily_business_metrics_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tier 3 Dynamic Table - Product Performance Metrics\n",
    "\n",
    "The second Tier 3 table is `product_performance_metrics`, which aggregates sales and profit data by product.\n",
    "\n",
    "This table provides:\n",
    "- Product dimensions (name, category, subcategory, brand)\n",
    "- Sales volume (order count, units sold)\n",
    "- Revenue and profit totals\n",
    "- Performance indicators (revenue per unit, profit per unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tier 3) Dynamic table 'product_performance_metrics created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build product_performance_metrics using Snowpark DataFrame API\n",
    "product_performance_metrics_df = session.table(\"tasty_bytes_db.analytics.order_fact\") \\\n",
    "    .group_by(\n",
    "        \"menu_item_id\",\n",
    "        \"menu_item_name\",\n",
    "        \"item_category\",\n",
    "        \"item_subcategory\",\n",
    "        \"truck_brand_name\",\n",
    "        \"menu_type\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        # Sales volume metrics\n",
    "        F.count_distinct(F.col(\"order_id\")).alias(\"order_count\"),\n",
    "        F.sum(F.col(\"quantity\")).alias(\"total_units_sold\"),\n",
    "        # Revenue and profit metrics\n",
    "        F.sum(F.col(\"line_total\")).alias(\"total_revenue\"),\n",
    "        F.sum(F.col(\"line_profit\")).alias(\"total_profit\"),\n",
    "        F.round(F.avg(F.col(\"unit_price\")), 2).alias(\"avg_unit_price\"),\n",
    "        F.round(F.avg(F.col(\"profit_margin_pct\")), 2).alias(\"avg_profit_margin_pct\"),\n",
    "        # Cost metrics\n",
    "        F.avg(F.col(\"cost_of_goods_usd\")).alias(\"avg_cogs\"),\n",
    "        F.avg(F.col(\"sale_price_usd\")).alias(\"standard_sale_price\"),\n",
    "        # Performance indicators\n",
    "        (F.sum(F.col(\"line_total\")) / F.call_builtin(\"NULLIF\", F.sum(F.col(\"quantity\")), F.lit(0))).alias(\"revenue_per_unit\"),\n",
    "        (F.sum(F.col(\"line_profit\")) / F.call_builtin(\"NULLIF\", F.sum(F.col(\"quantity\")), F.lit(0))).alias(\"profit_per_unit\")\n",
    "    )\n",
    "\n",
    "# Create the dynamic table using Snowpark DataFrame method\n",
    "product_performance_metrics_df.create_or_replace_dynamic_table(\n",
    "    name=\"tasty_bytes_db.analytics.product_performance_metrics\",\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    lag=\"DOWNSTREAM\"\n",
    ")\n",
    "\n",
    "print(\"(Tier 3) Dynamic table 'product_performance_metrics created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ...or build the dynamic table in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create product_performance_metrics dynamic table\n",
    "product_performance_metrics_query = \"\"\"\n",
    "SELECT\n",
    "    -- Product dimensions\n",
    "    menu_item_id,\n",
    "    menu_item_name,\n",
    "    item_category,\n",
    "    item_subcategory,\n",
    "    truck_brand_name,\n",
    "    menu_type,\n",
    "    -- Sales volume metrics\n",
    "    COUNT(DISTINCT order_id) AS order_count,\n",
    "    SUM(quantity) AS total_units_sold,\n",
    "    -- Revenue and profit metrics\n",
    "    SUM(line_total) AS total_revenue,\n",
    "    SUM(line_profit) AS total_profit,\n",
    "    ROUND(AVG(unit_price), 2) AS avg_unit_price,\n",
    "    ROUND(AVG(profit_margin_pct), 2) AS avg_profit_margin_pct,\n",
    "    -- Cost metrics\n",
    "    AVG(cost_of_goods_usd) AS avg_cogs,\n",
    "    AVG(sale_price_usd) AS standard_sale_price,\n",
    "    -- Performance indicators\n",
    "    SUM(line_total) / NULLIF(SUM(quantity), 0) AS revenue_per_unit,\n",
    "    SUM(line_profit) / NULLIF(SUM(quantity), 0) AS profit_per_unit\n",
    "FROM tasty_bytes_db.analytics.order_fact_sql\n",
    "GROUP BY\n",
    "    menu_item_id,\n",
    "    menu_item_name,\n",
    "    item_category,\n",
    "    item_subcategory,\n",
    "    truck_brand_name,\n",
    "    menu_type\n",
    "\"\"\"\n",
    "\n",
    "product_performance_metrics_dt = DynamicTable(\n",
    "    name=\"product_performance_metrics_sql\",\n",
    "    target_lag=DownstreamLag(),  # Refresh when upstream tables refresh\n",
    "    warehouse=\"PYDATA_LAB_WH\",\n",
    "    query=product_performance_metrics_query\n",
    ")\n",
    "\n",
    "root.databases[database_name].schemas[\"analytics\"].dynamic_tables.create(\n",
    "    product_performance_metrics_dt,\n",
    "    mode=CreateMode.or_replace\n",
    ")\n",
    "\n",
    "print(\"(Tier 3) Dynamic table 'product_performance_metrics_sql' created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Tier 3 Tables Using Snowpark\n",
    "\n",
    "Let's examine the aggregated metrics to ensure they're calculating correctly using Snowpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from daily_business_metrics:\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_DATE\"  |\"DAY_NAME\"  |\"TOTAL_ORDERS\"  |\"UNIQUE_CUSTOMERS\"  |\"TOTAL_REVENUE\"  |\"AVG_ORDER_VALUE\"  |\"TOTAL_PROFIT\"  |\"AVG_PROFIT_MARGIN_PCT\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2022-11-01    |Tue         |425886          |18644               |59793523.2500    |51.75              |8961352.6000    |58.66                    |\n",
      "|2022-10-31    |Mon         |412664          |15854               |58254392.2500    |51.90              |8705875.3000    |58.67                    |\n",
      "|2022-10-30    |Sun         |459105          |18020               |64367132.7500    |51.62              |9647234.4500    |58.68                    |\n",
      "|2022-10-29    |Sat         |465857          |18145               |65379979.0000    |51.71              |9797068.6500    |58.67                    |\n",
      "|2022-10-28    |Fri         |448874          |21603               |63159728.0000    |51.81              |9464734.6500    |58.67                    |\n",
      "|2022-10-27    |Thu         |439475          |18029               |61790017.0000    |51.78              |9256160.4500    |58.69                    |\n",
      "|2022-10-26    |Wed         |432436          |16650               |60760310.2500    |51.75              |9102695.2000    |58.69                    |\n",
      "|2022-10-25    |Tue         |431288          |20235               |60639247.2500    |51.76              |9080964.4000    |58.66                    |\n",
      "|2022-10-24    |Mon         |407548          |18816               |57434597.2500    |51.82              |8597204.9500    |58.68                    |\n",
      "|2022-10-23    |Sun         |454339          |15029               |63829794.0000    |51.74              |9563098.8000    |58.67                    |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check daily_business_metrics using Snowpark\n",
    "print(\"Sample data from daily_business_metrics:\")\n",
    "\n",
    "# Replace with daily_business_metrics_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.daily_business_metrics\") \\\n",
    "    .select(\"order_date\", \"day_name\", \"total_orders\", \"unique_customers\", \"total_revenue\", \"avg_order_value\", \"total_profit\", \"avg_profit_margin_pct\") \\\n",
    "    .order_by(F.col(\"order_date\").desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data from product_performance_metrics (top products by revenue):\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"MENU_ITEM_NAME\"            |\"ITEM_CATEGORY\"  |\"ORDER_COUNT\"  |\"TOTAL_UNITS_SOLD\"  |\"TOTAL_REVENUE\"  |\"TOTAL_PROFIT\"  |\"AVG_PROFIT_MARGIN_PCT\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|The King Combo              |Main             |12627298       |21563360            |431267200.0000   |172506880.0000  |40.00                    |\n",
      "|Tandoori Mixed Grill        |Main             |12097473       |20659415            |371869470.0000   |144615905.0000  |38.89                    |\n",
      "|Lean Chicken Tikka Masala   |Main             |12098077       |20660126            |351222142.0000   |144620882.0000  |41.18                    |\n",
      "|Spicy Miso Vegetable Ramen  |Main             |11064556       |18897578            |325983220.5000   |193700174.5000  |59.42                    |\n",
      "|Tonkotsu Ramen              |Main             |11064124       |18895904            |325954344.0000   |193683016.0000  |59.42                    |\n",
      "|Creamy Chicken Ramen        |Main             |11064017       |18894849            |325936145.2500   |174777353.2500  |53.62                    |\n",
      "|Combination Curry           |Main             |12103092       |20663277            |309949155.0000   |123979662.0000  |40.00                    |\n",
      "|Chicken Pot Pie Crepe       |Main             |11811193       |20171467            |302572005.0000   |181543203.0000  |60.00                    |\n",
      "|Lobster Mac & Cheese        |Main             |10426234       |17807243            |267108645.0000   |89036215.0000   |33.33                    |\n",
      "|Gyro Plate                  |Main             |12626831       |21562600            |258751200.0000   |86250400.0000   |33.33                    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check product_performance_metrics using Snowpark\n",
    "print(\"Sample data from product_performance_metrics (top products by revenue):\")\n",
    "\n",
    "# Replace with product_performance_metrics_sql if needed\n",
    "session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .select(\"menu_item_name\", \"item_category\", \"order_count\", \"total_units_sold\", \"total_revenue\", \"total_profit\", \"avg_profit_margin_pct\") \\\n",
    "    .order_by(F.col(\"total_revenue\").desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Dynamic Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic tables in 'analytics' schema:\n",
      "DAILY_BUSINESS_METRICS\n",
      "ORDERS_ENRICHED\n",
      "ORDER_FACT\n",
      "ORDER_ITEMS_ENRICHED\n",
      "PRODUCT_PERFORMANCE_METRICS\n"
     ]
    }
   ],
   "source": [
    "# List all dynamic tables in the analytics schema\n",
    "analytics_schema_ref = root.databases[\"tasty_bytes_db\"].schemas[\"analytics\"]\n",
    "dynamic_table_collection = DynamicTableCollection(analytics_schema_ref)\n",
    "dynamic_tables = dynamic_table_collection.iter()\n",
    "print(\"Dynamic tables in 'analytics' schema:\")\n",
    "for dt in dynamic_tables:\n",
    "    print(dt.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Incremental refresh for dynamic tables \n",
    "\n",
    "To demonstrate incremental refresh, we need a way to add new data to our raw tables. We'll create a stored procedure that generates synthetic orders.\n",
    "\n",
    "This procedure:\n",
    "- Takes a parameter for the number of orders to generate\n",
    "- Creates synthetic order headers with randomized values\n",
    "- Creates corresponding order details (line items)\n",
    "- Maintains referential integrity between orders and line items\n",
    "\n",
    "Note: Stored procedure creation currently requires SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stored procedure to generate demo orders\n",
    "session.sql(\"\"\"\n",
    "    CREATE OR REPLACE PROCEDURE tasty_bytes_db.raw.generate_demo_orders(num_rows INTEGER)\n",
    "    RETURNS STRING\n",
    "    LANGUAGE SQL\n",
    "    AS\n",
    "    $$\n",
    "    DECLARE\n",
    "        orders_before INTEGER;\n",
    "        orders_after INTEGER;\n",
    "        orders_inserted INTEGER;\n",
    "        details_before INTEGER;\n",
    "        details_after INTEGER;\n",
    "        details_inserted INTEGER;\n",
    "    BEGIN\n",
    "        -- Capture counts before insert\n",
    "        SELECT COUNT(*) INTO :orders_before FROM tasty_bytes_db.raw.order_header;\n",
    "        SELECT COUNT(*) INTO :details_before FROM tasty_bytes_db.raw.order_detail;\n",
    "        \n",
    "        -- Create temporary table with new order IDs to maintain referential integrity\n",
    "        CREATE OR REPLACE TEMPORARY TABLE new_orders AS\n",
    "        SELECT\n",
    "            (1000000 + UNIFORM(1, 999999, RANDOM()))::NUMBER(38,0) AS new_order_id,\n",
    "            oh.order_id AS original_order_id,\n",
    "            oh.truck_id,\n",
    "            oh.location_id,\n",
    "            oh.customer_id,\n",
    "            oh.discount_id,\n",
    "            oh.shift_id,\n",
    "            oh.shift_start_time,\n",
    "            oh.shift_end_time,\n",
    "            oh.order_channel,\n",
    "            DATEADD('day', DATEDIFF('day', oh.order_ts, CURRENT_DATE()), oh.order_ts) AS order_ts,\n",
    "            oh.served_ts,\n",
    "            oh.order_currency,\n",
    "            oh.order_amount * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS order_amount,\n",
    "            oh.order_tax_amount,\n",
    "            oh.order_discount_amount,\n",
    "            oh.order_total * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS order_total\n",
    "        FROM tasty_bytes_db.raw.order_header oh\n",
    "        WHERE oh.order_id IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT :num_rows;\n",
    "        \n",
    "        -- Insert synthetic order headers\n",
    "        INSERT INTO tasty_bytes_db.raw.order_header (\n",
    "            order_id, truck_id, location_id, customer_id, discount_id, shift_id,\n",
    "            shift_start_time, shift_end_time, order_channel, order_ts, served_ts,\n",
    "            order_currency, order_amount, order_tax_amount, order_discount_amount,\n",
    "            order_total\n",
    "        )\n",
    "        SELECT\n",
    "            new_order_id, truck_id, location_id, customer_id, discount_id, shift_id,\n",
    "            shift_start_time, shift_end_time, order_channel, order_ts, served_ts,\n",
    "            order_currency, order_amount, order_tax_amount, order_discount_amount,\n",
    "            order_total\n",
    "        FROM new_orders;\n",
    "        \n",
    "        -- Insert corresponding order details (line items)\n",
    "        INSERT INTO tasty_bytes_db.raw.order_detail (\n",
    "            order_detail_id, order_id, menu_item_id, discount_id, line_number,\n",
    "            quantity, unit_price, price, order_item_discount_amount\n",
    "        )\n",
    "        SELECT\n",
    "            (2000000 + UNIFORM(1, 9999999, RANDOM()))::NUMBER(38,0) AS order_detail_id,\n",
    "            no.new_order_id AS order_id,\n",
    "            od.menu_item_id,\n",
    "            od.discount_id,\n",
    "            od.line_number,\n",
    "            od.quantity,\n",
    "            od.unit_price * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS unit_price,\n",
    "            od.price * (0.8 + UNIFORM(0, 0.4, RANDOM())) AS price,\n",
    "            od.order_item_discount_amount\n",
    "        FROM new_orders no\n",
    "        INNER JOIN tasty_bytes_db.raw.order_detail od\n",
    "            ON no.original_order_id = od.order_id;\n",
    "        \n",
    "        -- Capture counts after insert\n",
    "        SELECT COUNT(*) INTO :orders_after FROM tasty_bytes_db.raw.order_header;\n",
    "        SELECT COUNT(*) INTO :details_after FROM tasty_bytes_db.raw.order_detail;\n",
    "        \n",
    "        orders_inserted := :orders_after - :orders_before;\n",
    "        details_inserted := :details_after - :details_before;\n",
    "        \n",
    "        -- Clean up temporary table\n",
    "        DROP TABLE IF EXISTS new_orders;\n",
    "        \n",
    "        RETURN 'Successfully generated ' || orders_inserted::STRING || ' new orders with ' ||\n",
    "               details_inserted::STRING || ' line items. Total orders: ' || orders_after::STRING;\n",
    "    END;\n",
    "    $$\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Stored procedure 'generate_demo_orders' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Initial State Using Snowpark\n",
    "\n",
    "Before we add new data, let's capture the current state of our raw tables using Snowpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial order_header count: 248,201,269\n",
      "Initial order_detail count: 673,655,465\n",
      "Latest order timestamp: 2022-11-01 22:59:59\n"
     ]
    }
   ],
   "source": [
    "# Get current row counts using Snowpark\n",
    "initial_order_count = session.table(\"tasty_bytes_db.raw.order_header\").count()\n",
    "initial_detail_count = session.table(\"tasty_bytes_db.raw.order_detail\").count()\n",
    "\n",
    "print(f\"Initial order_header count: {initial_order_count:,}\")\n",
    "print(f\"Initial order_detail count: {initial_detail_count:,}\")\n",
    "\n",
    "# Get latest order timestamp using Snowpark\n",
    "latest_order = session.table(\"tasty_bytes_db.raw.order_header\") \\\n",
    "    .select(F.max(\"order_ts\").alias(\"latest_order_ts\")) \\\n",
    "    .collect()[0][0]\n",
    "print(f\"Latest order timestamp: {latest_order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate New Demo Orders Using Stored Procedure\n",
    "\n",
    "Now we'll call the stored procedure to generate 1200 new orders. This simulates new data arriving in our system.\n",
    "\n",
    "We use Snowpark's `session.call()` method to invoke the stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01c0ea61-0207-38f6-0000-001979733429: 002141 (42601): SQL compilation error:\nUnknown user-defined function TASTY_BYTES_DB.RAW.GENERATE_DEMO_ORDERS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call stored procedure to generate 1200 new orders\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtasty_bytes_db.raw.generate_demo_orders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/utils.py:1148\u001b[0m, in \u001b[0;36mpublicapi.<locals>.call_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_emit_ast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m is_ast_enabled()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# TODO: Could modify internal docstring to display that users should not modify the _emit_ast parameter.\u001b[39;00m\n\u001b[0;32m-> 1148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/session.py:4490\u001b[0m, in \u001b[0;36mSession.call\u001b[0;34m(self, sproc_name, statement_params, block, log_on_exception, return_dataframe, _emit_ast, *args)\u001b[0m\n\u001b[1;32m   4429\u001b[0m \u001b[38;5;129m@publicapi\u001b[39m\n\u001b[1;32m   4430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m   4431\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4438\u001b[0m     _emit_ast: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   4439\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Any, AsyncJob]:\n\u001b[1;32m   4440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls a stored procedure by name.\u001b[39;00m\n\u001b[1;32m   4441\u001b[0m \n\u001b[1;32m   4442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4488\u001b[0m \u001b[38;5;124;03m        <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   4489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4491\u001b[0m \u001b[43m        \u001b[49m\u001b[43msproc_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4492\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_return_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4497\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_emit_ast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_emit_ast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/session.py:4612\u001b[0m, in \u001b[0;36mSession._call\u001b[0;34m(self, sproc_name, statement_params, is_return_table, log_on_exception, _emit_ast, block, *args)\u001b[0m\n\u001b[1;32m   4607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_return_table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4608\u001b[0m     is_return_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_is_return_table(\n\u001b[1;32m   4609\u001b[0m         sproc_name, \u001b[38;5;241m*\u001b[39margs, log_on_exception\u001b[38;5;241m=\u001b[39mlog_on_exception\n\u001b[1;32m   4610\u001b[0m     )\n\u001b[0;32m-> 4612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_sproc_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_return_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_return_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mast_stmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/session.py:4929\u001b[0m, in \u001b[0;36mSession._execute_sproc_internal\u001b[0;34m(self, query, is_return_table, block, statement_params, ast_stmt, log_on_exception)\u001b[0m\n\u001b[1;32m   4927\u001b[0m set_api_call_source(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession.call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4928\u001b[0m \u001b[38;5;66;03m# Note the collect is implicit within the stored procedure call, so should not emit_ast here.\u001b[39;00m\n\u001b[0;32m-> 4929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_emit_ast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/telemetry.py:295\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ResourceUsageCollector() \u001b[38;5;28;01mas\u001b[39;00m resource_usage_collector:\n\u001b[0;32m--> 295\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     resource_usage \u001b[38;5;241m=\u001b[39m resource_usage_collector\u001b[38;5;241m.\u001b[39mget_resource_usage()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/utils.py:1142\u001b[0m, in \u001b[0;36mpublicapi.<locals>.call_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_emit_ast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;66;03m# The caller provided _emit_ast explicitly.\u001b[39;00m\n\u001b[0;32m-> 1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_emit_ast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m is_ast_enabled()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# TODO: Could modify internal docstring to display that users should not modify the _emit_ast parameter.\u001b[39;00m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/dataframe.py:774\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self, statement_params, block, log_on_exception, case_sensitive, _emit_ast)\u001b[0m\n\u001b[1;32m    771\u001b[0m     _, kwargs[DATAFRAME_AST_PARAMETER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39m_ast_batch\u001b[38;5;241m.\u001b[39mflush(stmt)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_telemetry_context_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect, \u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_collect_with_tag_no_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/dataframe.py:847\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[0;34m(self, statement_params, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[1;32m    844\u001b[0m     statement_params \u001b[38;5;241m=\u001b[39m track_data_source_statement_params(\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28mself\u001b[39m, statement_params \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_statement_params\n\u001b[1;32m    846\u001b[0m     )\n\u001b[0;32m--> 847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcollect_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollect_stacktrace_in_query_tag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:648\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[0;34m(self, plan, to_pandas, to_iter, to_arrow, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    639\u001b[0m     is_in_stored_procedure()\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m ):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsync query is not supported in stored procedure yet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m result_set, result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_arrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_set\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:426\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    423\u001b[0m ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[1;32m    424\u001b[0m     e, debug_context\u001b[38;5;241m=\u001b[39mdebug_context\n\u001b[1;32m    425\u001b[0m )\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:179\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    174\u001b[0m     _enable_dataframe_trace_on_error,\n\u001b[1;32m    175\u001b[0m     _enable_trace_sql_errors_to_dataframe,\n\u001b[1;32m    176\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowpark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselect_statement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    182\u001b[0m         Selectable,\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:770\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, ignore_results, to_arrow, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m     kwargs[DATAFRAME_AST_PARAMETER] \u001b[38;5;241m=\u001b[39m dataframe_ast\n\u001b[1;32m    769\u001b[0m is_final_query \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(main_queries) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 770\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_final_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_post_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_arrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_final_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m placeholders[query\u001b[38;5;241m.\u001b[39mquery_id_place_holder] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    787\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mquery_id\n\u001b[1;32m    788\u001b[0m )\n\u001b[1;32m    789\u001b[0m result_meta \u001b[38;5;241m=\u001b[39m get_new_description(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor)\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:137\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    134\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:131\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    134\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    135\u001b[0m     )\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:544\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, to_arrow, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m         query_id_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:529\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, to_arrow, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m     cached_analyze_attributes\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 529\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_and_notify_query_listener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:459\u001b[0m, in \u001b[0;36mServerConnection.execute_and_notify_query_listener\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     err_query \u001b[38;5;241m=\u001b[39m ex\u001b[38;5;241m.\u001b[39mquery \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ex, Error) \u001b[38;5;28;01melse\u001b[39;00m query\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    457\u001b[0m         QueryRecord(sfqid, err_query, \u001b[38;5;28;01mFalse\u001b[39;00m), is_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnotify_kwargs\n\u001b[1;32m    458\u001b[0m     )\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    461\u001b[0m notify_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(results_cursor\u001b[38;5;241m.\u001b[39m_request_id)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    463\u001b[0m     QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnotify_kwargs\n\u001b[1;32m    464\u001b[0m )\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:450\u001b[0m, in \u001b[0;36mServerConnection.execute_and_notify_query_listener\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         notify_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe_uuid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m statement_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_PLAN_UUID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 450\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    452\u001b[0m     notify_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequestId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/connector/cursor.py:1121\u001b[0m, in \u001b[0;36mSnowflakeCursorBase.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1118\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1119\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1121\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/connector/errors.py:286\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    265\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    294\u001b[0m             error_class,\n\u001b[1;32m    295\u001b[0m             error_value,\n\u001b[1;32m    296\u001b[0m         )\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/connector/errors.py:341\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 341\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/pydata2025/.venv/lib/python3.12/site-packages/snowflake/connector/errors.py:217\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    215\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    218\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    219\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    220\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    221\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    222\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    225\u001b[0m     ),\n\u001b[1;32m    226\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    227\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    228\u001b[0m )\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1304): 01c0ea61-0207-38f6-0000-001979733429: 002141 (42601): SQL compilation error:\nUnknown user-defined function TASTY_BYTES_DB.RAW.GENERATE_DEMO_ORDERS."
     ]
    }
   ],
   "source": [
    "# Call stored procedure to generate 1200 new orders\n",
    "result = session.call(\"tasty_bytes_db.raw.generate_demo_orders\", 1200)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify New Data Using Snowpark\n",
    "\n",
    "Let's confirm the new data was inserted into our raw tables using Snowpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated row counts\n",
    "new_order_count = session.table(\"tasty_bytes_db.raw.order_header\").count()\n",
    "new_detail_count = session.table(\"tasty_bytes_db.raw.order_detail\").count()\n",
    "\n",
    "print(f\"New order_header count: {new_order_count:,} (+{new_order_count - initial_order_count:,} rows)\")\n",
    "print(f\"New order_detail count: {new_detail_count:,} (+{new_detail_count - initial_detail_count:,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Trigger Incremental Refresh - Tier 1\n",
    "\n",
    "Dynamic tables refresh automatically based on their TARGET_LAG, but we can manually trigger a refresh to see immediate results.\n",
    "\n",
    "We'll refresh the Tier 1 tables first, which will detect and process only the new data (incremental refresh) in the raw zone tables.\n",
    "\n",
    "Note: ALTER DYNAMIC TABLE commands currently require SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh Tier 1 tables\n",
    "# Append _sql if you want to refresh those created with SQL queries\n",
    "\n",
    "print(\"Refreshing orders_enriched...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.orders_enriched REFRESH\").collect()\n",
    "\n",
    "print(\"Refreshing order_items_enriched...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.order_items_enriched REFRESH\").collect()\n",
    "\n",
    "print(\"Tier 1 tables refreshed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Tier 1 Refresh History\n",
    "\n",
    "We can query the refresh history to see whether Snowflake performed an INCREMENTAL or FULL refresh.\n",
    "\n",
    "Incremental refresh processes only changed data, which is much faster and more efficient than a full refresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check refresh history for orders_enriched\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for orders_enriched:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time, refresh_trigger\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.ORDERS_ENRICHED'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check refresh history for order_items_enriched\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for order_items_enriched:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time, refresh_trigger\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.ORDER_ITEMS_ENRICHED'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Trigger Refresh - Tier 2 and Tier 3 Tables\n",
    "\n",
    "Now we'll refresh the downstream tables (Tier 2 and Tier 3). Because they use `TARGET_LAG = 'DOWNSTREAM'`, they will automatically refresh when their upstream dependencies change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh Tier 2 table\n",
    "# Append _sql if you want to refresh those created with SQL queries\n",
    "\n",
    "print(\"Refreshing order_fact...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.order_fact REFRESH\").collect()\n",
    "\n",
    "# Refresh Tier 3 tables\n",
    "print(\"Refreshing daily_business_metrics...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.daily_business_metrics REFRESH\").collect()\n",
    "\n",
    "print(\"Refreshing product_performance_metrics...\")\n",
    "session.sql(\"ALTER DYNAMIC TABLE tasty_bytes_db.analytics.product_performance_metrics REFRESH\").collect()\n",
    "\n",
    "print(\"All downstream tables refreshed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Refresh History for Tier 2 and Tier 3 tables\n",
    "\n",
    "Let's examine the refresh history for Tier 2 and Tier 3 tables to see the incremental refresh in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check refresh history for order_fact\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for order_fact:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.ORDER_FACT'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check refresh history for daily_business_metrics\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for daily_business_metrics:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.DAILY_BUSINESS_METRICS'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check refresh history for product_performance_metrics\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Refresh history for product_performance_metrics:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT name, refresh_action, state, refresh_start_time\n",
    "    FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(\n",
    "        NAME => 'tasty_bytes_db.ANALYTICS.PRODUCT_PERFORMANCE_METRICS'\n",
    "    ))\n",
    "    ORDER BY refresh_start_time DESC \n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Updated Metrics Using Snowpark\n",
    "\n",
    "Let's query our aggregated metrics using Snowpark to confirm that the new data has propagated through the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check updated daily metrics using Snowpark\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Updated daily business metrics (most recent dates):\")\n",
    "session.table(\"tasty_bytes_db.analytics.daily_business_metrics\") \\\n",
    "    .select(\"order_date\", \"total_orders\", \"total_items_sold\", \"total_revenue\", \"total_profit\") \\\n",
    "    .order_by(F.col(\"order_date\").desc()) \\\n",
    "    .limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check updated product performance using Snowpark\n",
    "# Append _sql to table name if you want to check those created with SQL queries\n",
    "\n",
    "print(\"Updated product performance (top products by revenue):\")\n",
    "session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .select(\"menu_item_name\", \"item_category\", \"total_units_sold\", \"total_revenue\", \"total_profit\") \\\n",
    "    .order_by(F.col(\"total_revenue\").desc()) \\\n",
    "    .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Latest Refresh Operations\n",
    "\n",
    "Let's create a summary view of the most recent refresh for each dynamic table, showing:\n",
    "- Refresh type (INCREMENTAL vs FULL)\n",
    "- State (SUCCEEDED, FAILED, etc.)\n",
    "- Duration in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of latest refresh operations\n",
    "print(\"Latest refresh operations for all dynamic tables:\")\n",
    "session.sql(\"\"\"\n",
    "    SELECT\n",
    "        name,\n",
    "        refresh_action,\n",
    "        state,\n",
    "        refresh_start_time,\n",
    "        refresh_end_time,\n",
    "        DATEDIFF('second', refresh_start_time, refresh_end_time) AS refresh_duration_seconds\n",
    "    FROM (\n",
    "        SELECT \n",
    "            name, \n",
    "            refresh_action, \n",
    "            state, \n",
    "            refresh_start_time, \n",
    "            refresh_end_time,\n",
    "            ROW_NUMBER() OVER (PARTITION BY name ORDER BY refresh_start_time DESC) as rn\n",
    "        FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY())\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "    ORDER BY name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Create an AI agent to extract insights from data in dynamic tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create infrastructure for agent\n",
    "\n",
    "We'll use Snowflake Intelligence to create and use AI agents that can answer natural language questions about your data. First, we'll create the necessary infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grant required privileges (requires ACCOUNTADMIN)\n",
    "session.use_role(\"ACCOUNTADMIN\")\n",
    "session.sql(\"GRANT CREATE DATABASE ON ACCOUNT TO ROLE pydata_lab_role\").collect()\n",
    "\n",
    "# Switch back to pydata_lab_role\n",
    "session.use_role(\"pydata_lab_role\")\n",
    "print(\"Privileges granted for Snowflake Intelligence\")\n",
    "\n",
    "# Create Intelligence database using Python API\n",
    "intelligence_db = Database(name=\"snowflake_intelligence\")\n",
    "root.databases.create(intelligence_db, mode=CreateMode.or_replace)\n",
    "print(\"Database 'snowflake_intelligence' created\")\n",
    "\n",
    "# Create agents schema using Python API\n",
    "intelligence_db_ref = root.databases[\"snowflake_intelligence\"]\n",
    "agents_schema = Schema(name=\"agents\")\n",
    "intelligence_db_ref.schemas.create(agents_schema, mode=CreateMode.or_replace)\n",
    "print(\"Schema 'agents' created\")\n",
    "\n",
    "# Grant permissions\n",
    "session.sql(\"GRANT USAGE ON DATABASE snowflake_intelligence TO ROLE pydata_lab_role\").collect()\n",
    "session.sql(\"GRANT USAGE ON SCHEMA snowflake_intelligence.agents TO ROLE pydata_lab_role\").collect()\n",
    "session.sql(\"GRANT CREATE AGENT ON SCHEMA snowflake_intelligence.agents TO ROLE pydata_lab_role\").collect()\n",
    "\n",
    "print(\"Agent infrastructure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk through the UI with the instructor to create an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Questions for Intelligence Agent\n",
    "\n",
    "Once you create an agent through the UI and connect it to your semantic model, you can ask questions like:\n",
    "\n",
    "1. What was the total revenue for the last 30 days?\n",
    "2. Which products have the highest profit margins?\n",
    "3. Show me daily revenue trends as a line chart\n",
    "4. How many unique customers did we have yesterday?\n",
    "5. What percentage of orders include discounts?\n",
    "6. Compare revenue by product category\n",
    "7. Show me the top 5 products by profit\n",
    "8. What are the busiest hours for orders?\n",
    "9. How does revenue vary by day of week?\n",
    "10. Which truck brands generate the most profit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the agent responses with the following queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 products by revenue using Snowpark\n",
    "print(\"Top 10 products by revenue (using Snowpark):\")\n",
    "top_products = session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .select(\n",
    "        F.col(\"menu_item_name\"),\n",
    "        F.col(\"item_category\"),\n",
    "        F.col(\"total_revenue\"),\n",
    "        F.col(\"total_profit\"),\n",
    "        F.col(\"avg_profit_margin_pct\")\n",
    "    ) \\\n",
    "    .order_by(F.col(\"total_revenue\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate revenue by category using Snowpark aggregation\n",
    "print(\"Revenue by product category (using Snowpark):\")\n",
    "category_revenue = session.table(\"tasty_bytes_db.analytics.product_performance_metrics\") \\\n",
    "    .group_by(\"item_category\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "        F.sum(\"total_profit\").alias(\"category_profit\"),\n",
    "        F.sum(\"total_units_sold\").alias(\"units_sold\")\n",
    "    ) \\\n",
    "    .order_by(F.col(\"category_revenue\").desc())\n",
    "\n",
    "category_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily revenue trend for last 30 days using Snowpark\n",
    "print(\"Daily revenue trend (last 30 days):\")\n",
    "daily_trend = session.table(\"tasty_bytes_db.analytics.daily_business_metrics\") \\\n",
    "    .select(\n",
    "        F.col(\"order_date\"),\n",
    "        F.col(\"total_orders\"),\n",
    "        F.col(\"total_revenue\"),\n",
    "        F.col(\"total_profit\"),\n",
    "        F.col(\"unique_customers\")\n",
    "    ) \\\n",
    "    .order_by(F.col(\"order_date\").desc()) \\\n",
    "    .limit(30)\n",
    "\n",
    "daily_trend.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "When you're done with this lab, you can clean up all resources.\n",
    "\n",
    "WARNING: This will delete all data and objects created in this notebook. Only run this cell if you're sure you want to remove everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to perform cleanup\n",
    "\n",
    "# # Drop databases\n",
    "# root.databases[\"tasty_bytes_db\"].delete()\n",
    "# print(\"Database 'tasty_bytes_db' dropped\")\n",
    "\n",
    "# root.databases[\"snowflake_intelligence\"].delete()\n",
    "# print(\"Database 'snowflake_intelligence' dropped\")\n",
    "\n",
    "# # Drop warehouse\n",
    "# root.warehouses[\"PYDATA_LAB_WH\"].delete()\n",
    "# print(\"Warehouse 'PYDATA_LAB_WH' dropped\")\n",
    "\n",
    "# # Optionally drop the role\n",
    "# # session.use_role(\"ACCOUNTADMIN\")\n",
    "# # session.sql(\"DROP ROLE IF EXISTS pydata_lab_role\").collect()\n",
    "# # print(\"Role 'pydata_lab_role' dropped\")\n",
    "\n",
    "print(\"Cleanup section ready (uncomment to execute)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the session\n",
    "session.close()\n",
    "print(\"Session closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review and Conclusion\n",
    "\n",
    "Let's recap what we've built:\n",
    "\n",
    "1. **End-to-end data pipeline** that:\n",
    "\n",
    "- Ingests raw data from CSV files in an S3 bucket and lands it as data in tables\n",
    "- Transforms data automatically with declarative dynamic tables that process only incremental changes\n",
    "- Delivers a semantic model that can be used for an AI agent to extract insights from data\n",
    "\n",
    "2. **No Orchestration Code**: No need for tasks, custom scheduling logic, or third-party scheduling tools\n",
    "\n",
    "### What you used to build the pipeline\n",
    "\n",
    "- Cloud object storage (AWS S3) to hold raw data files\n",
    "\n",
    "- **Snowflake Python APIs** (`snowflake.core`) for creating databases, schemas, warehouses, and tables, and for ingesting data into Snowflake\n",
    "\n",
    "- **Snowpark DataFrames** for querying data\n",
    "\n",
    "- (Some) **SQL** only when necessary (dynamic tables, stored procedures, stages) where Snowflake Python API support is limited\n",
    "\n",
    "- Semantic models (YAML files) to configure an AI agent for data insights\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- Snowflake Developer Hub: [developers.snowflake.com](developers.snowflake.com)\n",
    "\n",
    "- **Snowflake-Labs**, open source GitHub organization: [https://github.com/Snowflake-Labs](https://github.com/Snowflake-Labs)\n",
    "\n",
    "- [Snowflake Developers YouTube Channel](https://www.youtube.com/channel/UCxgY7r-o_ql8ADIdyiQr3Zw)\n",
    "\n",
    "- [**Coursera: Snowflake Data Engineering Professional Certificate:**](https://www.coursera.org/professional-certificates/snowflake-data-engineering)\n",
    "\n",
    "- [Snowflake Dynamic Tables Documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)\n",
    "\n",
    "- [Snowpark Python Developer Guide](https://docs.snowflake.com/en/developer-guide/snowpark/python/index)\n",
    "\n",
    "- [Snowflake Python API Reference](https://docs.snowflake.com/developer-guide/snowflake-python-api/reference/latest/index)\n",
    "\n",
    "- [Snowflake Intelligence](https://docs.snowflake.com/en/user-guide/snowflake-cortex/snowflake-intelligence)\n",
    "\n",
    "### Connect with me on LinkedIn\n",
    "\n",
    "[https://www.linkedin.com/in/gilberto-hernandez](https://www.linkedin.com/in/gilberto-hernandez)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
