# โ๏ธ From Notebook to Pipeline: Hands-On Data Engineering with Python

[![PyData Boston 2025](https://img.shields.io/badge/PyData-Boston%202025-blue)](https://pydata.org/)
[![Snowflake](https://img.shields.io/badge/Powered%20by-Snowflake-29B5E8)](https://www.snowflake.com/)
[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Author:** Renan Peres  
> **Event:** PyData Boston 2025

---

## ๐ Table of Contents

- [Overview](#-overview)
- [What You'll Learn](#-what-youll-learn)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Pipeline Architecture](#-pipeline-architecture)
- [Resources](#-resources)

---

## ๐ฏ Overview

This repository contains the companion notebook for the **"From Notebook to Pipeline: Hands-On Data Engineering with Python"** tutorial at PyData Boston 2025.

Build a complete, production-ready data pipeline using Python and Snowflakeโtransforming raw CSV data into actionable business insights with automated incremental refreshes.

### ๐ Quick Links

| Resource | Link |
|----------|------|
| ๐ GitHub Repository | [Snowflake-Labs/pydata_boston_2025_notebook_to_pipeline](https://github.com/Snowflake-Labs/pydata_boston_2025_notebook_to_pipeline) |
| โ๏ธ Snowflake Free Trial (120 days) | [Sign Up Here](https://signup.snowflake.com/?trial=student&cloud=aws&region=us-west-2) |
| ๐ฅ Workshop Tutorial: Hands-On Data Engineering with Python  | [Youtube Video](https://www.youtube.com/watch?v=Rj4_atYG3MY&list=PLGVZCDnMOq0rhHGZp727T3aRoUYwXXxQJ&index=6) |
| ๐ฅ Tutorial: Create External Users in Snowflake | [Youtube Video](https://www.youtube.com/watch?v=TqDXlVwoz1c) |

---

## ๐ What You'll Learn

By completing this tutorial, you will be able to:

- โ **Ingest data** from CSV files stored in cloud storage (AWS S3) into Snowflake tables
- โ **Transform raw data** using both Python (Snowpark) and SQL approaches
- โ **Build a 3-tier automated pipeline** using Snowflake Dynamic Tables
- โ **Leverage incremental refresh** to process only changed data efficiently
- โ **Monitor pipeline performance** through refresh history and metrics
- โ **Create AI-powered insights** using semantic models and Snowflake Intelligence *(optional)*

---

## ๐ Prerequisites

| Requirement | Details |
|-------------|---------|
| **Python** | Version 3.10.x or higher |
| **Package Manager** | pip or conda |
| **IDE** | VS Code with Jupyter extension (recommended) |
| **Snowflake Account** | [Free 120-day trial](https://tinyurl.com/pydata2025) |
| **Internet Connection** | Required for Snowflake connectivity |

### Required Python Packages

```bash
snowflake-snowpark-python>=1.11.0
snowflake-core>=0.6.0
pandas>=1.5.0
python-dotenv>=1.0.0  # for loading .env into the notebook
```

---

## ๐ Quick Start

### Option A: Local Development (Intermediate/Advanced)

#### Using pip (classic)
```bash
# 1. Clone the repository
git clone https://github.com/Snowflake-Labs/pydata_boston_2025_notebook_to_pipeline.git
cd pydata_boston_2025_notebook_to_pipeline

# 2. Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Verify installation
python -c "from snowflake.snowpark import Session; from snowflake.core import Root; print('โ Installation successful!')"

# 5. Open the notebook in VS Code and start coding!
```

#### Using uv (fast Python package manager)
```bash
# 0. Install uv (Linux/macOS)
curl -LsSf https://astral.sh/uv/install.sh | sh
# Windows users: see https://docs.astral.sh/uv/installing/ for options

# 1. Clone the repository
git clone https://github.com/Snowflake-Labs/pydata_boston_2025_notebook_to_pipeline.git
cd pydata_boston_2025_notebook_to_pipeline

# 2. Create a virtual environment with uv
uv venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies (reads requirements.txt)
uv pip install -r requirements.txt

# 4. Verify installation (without activating, you can also do: `uv run ...`)
python -c "from snowflake.snowpark import Session; from snowflake.core import Root; print('โ Installation successful!')"
# Or
uv run -c "from snowflake.snowpark import Session; from snowflake.core import Root; print('โ Installation successful!')"

# 5. Open the notebook in VS Code and start coding!
```

### Option B: Snowflake Notebooks (Beginner-Friendly)

1. Log into your [Snowflake account](https://app.snowflake.com/)
2. Navigate to **Projects** โ **Notebooks**
3. Click **+ Notebook** โ **Import .ipynb file**
4. Configure: Database: `SNOWFLAKE_LEARNING_DB`, Schema: `PUBLIC`, Runtime: `Run on warehouse`
5. Install packages via the **Packages** dropdown: `snowflake`, `snowflake-snowpark-python`

---

## ๐ Project Structure

```
data_engineering_snowflake_pipeline/
โโโ ๐ cloud_data_pipeline.ipynb    # Main tutorial notebook
โโโ ๐ README.md                    # This file
โโโ ๐ requirements.txt             # Python dependencies
โโโ ๐ data/                        # Sample data files (if any)
โโโ ๐ img/                         # Documentation images
โโโ .env                            # Local-only credentials (not committed)
```

> Tip: Add `.env` to your `.gitignore` to avoid committing secrets.

---

## ๐ Environment Variables (.env)

Create a local `.env` file to avoid hardcoding Snowflake credentials:

```ini
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ROLE=ACCOUNTADMIN
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
```

Use `python-dotenv` in the notebook to load these values:

```python
from dotenv import load_dotenv
import os

load_dotenv()
connection_parameters = {
    "account": os.getenv("SNOWFLAKE_ACCOUNT"),
    "user": os.getenv("SNOWFLAKE_USER"),
    "password": os.getenv("SNOWFLAKE_PASSWORD"),
    "role": os.getenv("SNOWFLAKE_ROLE"),
    "warehouse": os.getenv("SNOWFLAKE_WAREHOUSE"),
}
```

---

## ๐ Pipeline Architecture

You'll build a pipeline for **Tasty Bytes**, a global food truck company that needs fresh, daily business metrics.

### Data Flow Diagram

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                              DATA PIPELINE                                   โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ                                                                             โ
โ  โโโโโโโโโโโโโโโโโ                                                          โ
โ  โ   RAW ZONE    โ  โ CSV files from AWS S3                                 โ
โ  โโโโโโโโโโโโโโโโโฃ                                                          โ
โ  โ order_header  โ  ~248M rows                                              โ
โ  โ order_detail  โ  ~618M rows                                              โ
โ  โ menu          โ  ~287 rows                                               โ
โ  โโโโโโโโโคโโโโโโโโ                                                          โ
โ          โ                                                                  โ
โ          โผ                                                                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ  โ           TIER 1: ENRICHMENT          โ  โ 12-hour lag                   โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ                                  โ
โ  โ orders_enriched      โ Temporal dims, โ                                  โ
โ  โ                      โ discount flags โ                                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ  โ order_items_enriched โ Product info,  โ                                  โ
โ  โ                      โ profit calcs   โ                                  โ
โ  โโโโโโโโโโโโโโโโโคโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ                  โ                                                          โ
โ                  โผ                                                          โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ  โ         TIER 2: FACT TABLE            โ  โ Downstream lag                โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ                                  โ
โ  โ order_fact           โ Unified view   โ                                  โ
โ  โ                      โ of all orders  โ                                  โ
โ  โโโโโโโโโโโโโโโโโคโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ                  โ                                                          โ
โ                  โผ                                                          โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ  โ      TIER 3: AGGREGATED METRICS       โ  โ Downstream lag                โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ                                  โ
โ  โ daily_business_metrics โ Revenue,     โ                                  โ
โ  โ                        โ orders, etc. โ                                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ  โ product_performance    โ Sales,       โ                                  โ
โ  โ _metrics               โ profit/item  โ                                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                                  โ
โ                                                                             โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

### Pipeline Tiers Explained

| Tier | Purpose | Tables | Refresh Strategy |
|------|---------|--------|------------------|
| **Raw** | Land source data | `order_header`, `order_detail`, `menu` | Manual/Batch |
| **Tier 1** | Enrich & clean | `orders_enriched`, `order_items_enriched` | 12-hour lag |
| **Tier 2** | Join & unify | `order_fact` | Downstream |
| **Tier 3** | Aggregate | `daily_business_metrics`, `product_performance_metrics` | Downstream |

---

## ๐ Resources

### Documentation
- [Snowflake Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-intro)
- [Snowpark Python Developer Guide](https://docs.snowflake.com/en/developer-guide/snowpark/python/index)
- [Snowflake Python API Reference](https://docs.snowflake.com/developer-guide/snowflake-python-api/reference/latest/index)
- [Snowflake Intelligence](https://docs.snowflake.com/en/user-guide/snowflake-cortex/snowflake-intelligence)

### Learning
- [Coursera: Snowflake Data Engineering Professional Certificate](https://www.coursera.org/professional-certificates/snowflake-data-engineering)
- [Snowflake Developers YouTube Channel](https://www.youtube.com/channel/UCxgY7r-o_ql8ADIdyiQr3Zw)
- [Snowflake Developer Hub](https://developers.snowflake.com)

### Community
- [Snowflake-Labs GitHub](https://github.com/Snowflake-Labs)